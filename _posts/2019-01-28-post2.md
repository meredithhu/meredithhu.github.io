---
layout: post
title: A Survey - Marketing Research and Computer Vision, Part II
date: 2019-01-28
---


Marketing Applications of Computer Vision

A.K.A. Computer vision research papers on marketing problems


<h4> Advertising </h4>

Joo, Li, Steen and Zhu (2014) first introduce the problem of understanding visual persuasion in computer vision. A persuasive image has an underlying intention to persuade the viewer by its visuals and is widely used in mass media, such as TV news, advertisements, and political campaigns. Joo et al. (2014) focus on understanding the underlying intents of such persuasive images, for which they collected a a dataset of 1,124 images of politicians labeled with ground-truth intents by ranking. They identify twelve syntactical features as predictors and nine dimensions of communicative intents as labels. The communicative intents are grouped into three buckets:
1. Emotional Traits: Happy, Angry, Fearful;
2. Personality Traits and Values: Competent, Energetic, Comforting, Trustworthy, So- cially Dominant;
3. Overall Favorability: Favorable.
As are syntactical features:
1. Facial Display: Smile, Look Down, Eye Open, Mouth Open;
2. Body Cues - Gestures: Hand-Wave, Hand-Shake, Finger-point, Touch-Head, Hug; 3. Scene Context: Large Crowd, Dark-Background, Indoor.

Hussain, Zhang, Zhang, Ye, Thomas, Agha, Ong and Kovashka (2017) create two datasets: an image dataset of 64,832 image ads and a video dataset of 3,477 ads, both of which con- tain rich annotations of the topics, the sentiments, questions and answers describing the objectives, the reasoning the ad presents to persuade the viewer, as well as the symbolic references ads make. The authors develop computer vision system to understand these ads and evaluate it on tasks such as symbolic question-answering, topic and sentiment recog- nition. The highest accuracy they achieved on symbolism prediction is 50% on the image dataset, and the accuracies for topic and sentiment predictions on video ads were 35.1% and 32.8%, respectively. They also predicted whether the videos ads were funny or exciting — the resulting accuracies were 78.6% and 78.2% accordingly. More interesting applications could build on the result from Hussain et al. (2017). For instance,
• methods could be developed to predict how effective a certain ad will be given a target audience;
• on the part of viewers, the automatic ad-understanding system could help prevent being tricked into buying certain products;
• by decoding the messages of ads, better ad-targeting strategies could be tailored ac- cording to user interests;
• it could be extended to generate automatic ad descriptions and summarizations.

Azimi, Zhang, Zhou, Navalpakkam, Mao and Fern (2012) study the relationship between the visual appearance and performance of creatives using large scale data in the worlds (then) largest display ads exchange system, RightMedia. They design a set of 43 visual features, categorized into three different sets:
• Global Features: Grey Level Features, Color Distributions, Model-Based Color Har- mony, Color Coherence, Hue Distribution, Lightness Features;
• Local Features: Segment Size, Segment Hues, Segment Color Harmony, Segment Light- ness;
• Advanced Features: Saliency Features, Number of Characters, Number of Faces.
They extract the visual features and conduct a series of experiments to evaluate the ef- fectiveness of visual features for click through rate prediction, ranking and performance classification. Based on the evaluation results, they selected a subset of features that have the most important impact on click through rates, useful for ads selection and developing visually appealing creatives.

State-of-the-art prediction algorithms for click-through rates on non-guaranteed display advertising rely heavily on historical information collected for advertisers, users and publish- ers, which makes it challenging when new advertisements are encountered due to the lack of historical data. Cheng, Zwol, Azimi, Manavoglu, Zhang, Zhou and Navalpakkam (2012) propose to mitigate this problem by integrating multimedia features extracted from display ads into the click prediction models because multimedia features can help capture the at- tractiveness of the ads with similar contents or aesthetics. The authors evaluate the use of numerous multimedia features, in addition to commonly used user, advertiser and publisher features, and demonstrate that adding multimedia features can significantly improve the accuracy of click prediction for new ads, compared to a baseline model.

Chilton (2018) tackle the problem of creative ad generation given a central message to be conveyed to the target audience. For instance, given the message “Smoking kills you”, their system outputs an image that blends two visual symbols — one represents the subject (“smoking”) and one represents the predicate (“kills you”) — a handgun loaded with cigarettes. More details to be found in their draft yet to be released.

McDuff, El Kaliouby, Cohn and Picard (2015) collected over 12,0000 facial responses from 1,223 people to 170 ads from a range of markets and product categories. The facial responses were automatically coded frame by frame at a large scale. Detected expressions were found sparse but aggregate responses revealed rich emotion trajectories. Ad liking were predicted accurately from webcam facial responses at a ROC AUC at 0.85, followed by a change in purchase intent at a ROC AUC at 0.78. They found that ad liking is driven by mostly positive expressions, whereas determinants of purchase intent are more complex: peak positive responses that are immediately preceded by a brand appearance are more likely to be effective.

Mei, Li, Hua and Li (2012) presents a contextual advertising system driven by images, which automatically associates relevant ads with an image rather than the entire text in a Web page and seamlessly inserts the ads in the non-intrusive areas within each individual image. The proposed system, called ImageSense, supports scalable advertising of, from root to node, Web sites, pages, and images. In ImageSense, the ads are selected based on not only textual relevance but also visual similarity, so that the ads yield contextual relevance to both the text in the Web page and the image content. The ad insertion positions are detected based on image salience, as well as face and text detection, to minimize intrusiveness to the user. They collected a unique advertisement-image dataset that consists of 7,285 unique ad product logos with annotations of 32,480 unique ad words done by 20 subjects, as well as 382,371 images from http://www.tango.msra and 200,000 images from Flickr, based on which they evaluate ImageSense with 1,100 ad triggering pages (100 web pages from major news sites, 1,000 images searched by top 100 image queries), and demonstrate the effectiveness of ImageSense for online image advertising.

Yadati, Katti and Kankanhalli (2014) propose an in-stream video advertising strategy for advertising platforms such as Youtube, which they term as Computational Affective Video- in-Video Advertising (CAVVA). They focus on the role emotions play in influencing the buying behavior of users and therefore factor in the emotional impact of the videos as well as advertisements. Given a video and a set of advertisements, their method first identify candidate advertisement insertion points, as well as the suitable advertisements based on marketing and consumer psychology theories. The two stage problem is then integrated as a single optimization function in a non-linear 01 integer programming framework and a so- lution was identified based on genetic algorithm. They evaluate CAVVA using a subjective user-study and eye-tracking experiment. They are able to demonstrate that CAVVA strikes a balance between the conflicting goals of (a) minimizing the user disturbance because of advertisement insertion while (b) enhancing the user engagement with the advertising con- tent. Their method is pitted against existing advertising strategies and shown that CAVVA can enhance the users experience and also help increase the monetization potential of the advertising content.

Sanchez, Binefa and Vitria (2002) approach the problem of automatic TV commercials recognition, and introduce an algorithm for scene break detection. The structure of each commercial is represented by the set of its key-frames, which areautomatically extracted from the video stream. The commonly used shot boundary detection techniques are based on individual image features or visual cues, which show significant performance lacks when they are applied to complex video content domains like commercials. The authors present a scene break detection algorithm based on the combined analysis of edge and color features. Local motion estimation is applied to each edge in a frame, and the continuity of the color around them is then checked in the following frame and thus the continuous presence of the objects and/or the background of the scene during each shot is essential for the proposed algorithm. They show that this approach outperforms single feature algorithms in terms of precision and recall.

Zhao, Yuan, Xu and Wu (2011) propose a data-mining method for thematic object discovery in commercials by finding spatially collocated visual features, such as logos, human facial reactions, ad placement and recognition, and detecting logos, are quite distinct from our goal of decoding the messages of ads.

<h4> Storytelling </h4>

Rohrbach, Rohrbach, Tandon and Schiele (2015) introduce a dataset of transcribed Descrip- tive video service (DVS) that is temporally aligned to full length HD movies. DVS provides linguistic descriptions of movies and are by design mainly visual. In addition, the aligned movie scripts were also collected for comparison between the two different sources of de- scriptions. In total the Movie Description dataset contains a parallel corpus of over 54,000 sentences and video snippets from 72 HD movies. Comparing DVS to scripts, they find that DVS is far more visual and describes precisely what is shown rather than what should happen according to the scripts created prior to movie production. Building on Rohrbach et al. (2015), Tapaswi, Zhu, Stiefelhagen, Torralba, Urtasun and Fidler (2016) introduce the MovieQA dataset which aims to evaluate automatic story comprehension from both video and text. The dataset consists of 14,944 questions about 408 movies with high semantic diversity. The questions range from simpler “Who” did “What” to “Whom”, to “Why” and “How” certain events occurred. Each question comes with a set of five possible answers; a correct one and four deceiving answers provided by human annotators. Multiple sources of information video clips, plots, subtitles, scripts, and DVS (Rohrbach et al. 2015) were all included. The authors extended existing QA techniques to show that question-answering with such open-ended semantics is hard and much future work awaits in this challenging domain.

Humor is a highly-valued human skill — arguably a sign of intelligence and creativity. Humor creation is a long-standing problem in Artificial Intelligence, because it does not eas- ily decompose and it cannot readily be defined or detected. Chandrasekaran, Vijayakumar, Antol, Bansal, Batra, Lawrence Zitnick and Parikh (2016) document progress in understand- ing subtleties of human expressions such as humor. They collected two datasets of abstract scenes that facilitate the study of humor at both the scene-level and the object-level. They annotated the funny scenes and explore the different types of humor depicted in them. By designing computational models that predict the funniness and alter the funniness of a scene, they were able to provide answers to questions such as “what content in a scene causes it to be funny?” They show that their models perform well quantitatively, and qualitatively through human studies. In the same vein, Chilton, Landay and Weld (2018) surveyed pro- fessional comedians and found evidence that the humor-generation process can be described. Based on this survey, they performed an analysis of news satire from The Onion and de- composed the process of humor creation into seven microtasks — aspect, expected reactions, expected reasons, associations, expectations violation mechanisms, beliefs, and evaluation. They then developed a workflow that invokes these microtasks dynamically. They validated and evaluated the microtasks and workflow with 20 human subjects.


<h4> Face Detection </h4>

Joo, Steen and Zhu (2015) design a fully automated system that can infer the perceived traits of a person from his face — social dimensions, such as “intelligence”, “honesty” and “competence” — and how those traits can be used to predict the outcomes of political elec- tions, job hires, and marriage engagements. The authors propose a hierarchical model for enduring traits inferred from faces, incorporating high-level perceptions and intermediate- level attributes. Surprisingly, or not so surprisingly, they show that the trained model can successfully classify the outcomes of two important political events, only using the pho- tographs of politicians’ faces. Firstly, it classifies the winners of a series of U.S. elections with the accuracy of 67.9% (Governors) and 65.5% (Senators). By interpreting the pipeline, they find that different political offices require different types of preferred traits. Secondly, the model can categorize the political party affiliations of politicians, i.e., Democrats vs. Republicans, with the accuracy of 62.6% (male) and 60.1% (female). This study appears to be the first to use automated visual trait analysis to predict the outcomes of real-world social events. Further, the proposed approach is more scalable and objective than the prior behavioral studies, and opens for a range of new exciting applications.

Huang and Kovashka (2016) extend Joo et al. (2015) by exploring a variety of features for predicting communicative intents. They study a number of facial expressions and body poses as cues for the implied nuances of the politician’s personality, as well as how the environmental settings such as kitchen or hospital influence the audience’s perception of the portrayed politician. They improve the performance by learning intermediate cues using convolutional neural networks and document state-of-the-art results on the Visual Persuasion dataset of Joo et al. (2015).

<h4> Infographics </h4>

Bylinskii, Kim, Donovan, Alsheikh, Madan, Pfister, Durand, Russell and Hertzmann (2017a) introduce automated models that predict the relative importance of different elements in data visualizations and graphic designs. They use neural networks trained on human clicks and importance annotations on hundreds of designs. The authors collected a dataset of crowdsourced importance, and analyzed the predictions of their neural network models with respect to ground truth importance and human eye movements. Therefore they demon- strate how such predictions of importance can be used for automatic design retargeting and thumbnailing. The validity of their method was established by user studies with hundreds of MTurk participants. With limited post-processing, the proposed importance-driven ap- plications are on par with, or outperform, state-of-the-art methods, including natural image saliency. The importance predictions can also be built into interactive design tools to offer immediate feedback during the design process.

Bylinskii, Alsheikh, Madan, Recasens, Zhong, Pfister, Durand and Oliva (2017b) intro- duce the problem of visual hashtag discovery for infographics: extracting visual elements from an infographic that are diagnostic of its topic. Based on a curated dataset of 29K large infographic images sampled across 26 categories and 391 tags, the proposed automated method consists of two steps: first, extract the text from an infographic and use it to pre- dict text tags indicative of the infographic content; second, use these predicted text tags as a supervisory signal to localize the most diagnostic visual elements from within the info- graphic, which they call “visual hashtags”. They exemplify how textual and visual elements can be used to jointly reason about the high-level topics (categories) of infographics, as well as the finer-grained sub-topics (tags). In addition, they demonstrate the power of text fea- tures in disambiguating and providing context for visual features. Their performances on a categorization and multi-label tag prediction were compared to human annotations.

<h4> Style Detection </h4>

There is a fast growing body of research that aims at learning a notion of styles from images, whether it be art (Karayev, Trentacoste, Han, Agarwala, Darrell, Hertzmann and Winnemoeller 2013, Liu, Yan, Ricci, Yang, Han, Winkler and Sebe 2015) (including the explosion of studies on style transfer, following the seminal work of Gatys, Ecker and Bethge (2015) or Gatys, Ecker and Bethge (2016), which we review in Section 3.6.2), vehicle (Jae Lee, Efros and Hebert 2013), scenic spots (Doersch, Singh, Gupta, Sivic and Efros 2012, Quercia, O’Hare and Cramer 2014), photograph (Thomas and Kovashka 2016), and clothing (Bossard, Dantone, Leistner, Wengert, Quack and Van Gool 2012, Veit, Kovacs, Bell, McAuley, Bala and Belongie 2015, Kiapour, Yamaguchi, Berg and Berg 2014), which we review below.

Learning styles of art. Garces, Agarwala, Gutierrez and Hertzmann (2014) present a method for measuring the similarity in style between two pieces of vector art, independent of content. Similarity is measured by the differences between four types of features: color, shading, texture, and stroke. Feature weightings are learned from crowdsourced experiments. This perceptual similarity enables style-based search, with which they demonstrate an ap- plication that allows users to create stylistically-coherent clip art mash-ups. (Karayev et al. 2013, Liu et al. 2015)
Learning city icons. Doersch et al. (2012) seek to automatically find visual elements that are most distinctive for a certain geo-spatial area, for example the city of Paris, given a large repository of geotagged imagery. A discriminative clustering approach is proposed to show that geographically representative image elements can be discovered automatically from Google Street View imagery. They demonstrate that these elements are visually in- terpretable and perceptually geo-informative, for the purpose of tourism marketing of Paris. The discovered visual elements can also support a variety of computational geography tasks, such as mapping architectural correspondences and influences within and across cities, find- ing representative elements at different geo-spatial scales, and geographically-informed image retrieval.

Quercia et al. (2014) present a crowdsourcing project that aims to investigate, at scale, which visual aspects of a city neighborhoods (e.g. London) make them appear beautiful, quiet, and/or happy. They collected votes from over 3.3K individuals and translate them into quantitative measures of urban perception, thereby quantifying each neighborhood’s aesthetic capital. By then using state-of-the-art image processing techniques, the authors were able to determine visual cues that may cause a street to be perceived as being beautiful, quiet, or happy. Effects of color, texture and visual words were identified. For example, the amount of greenery is the most positively associated visual cue with each of three qualities; by contrast, broad streets, fortress-like buildings, and council houses tend to be associated with the opposite qualities (ugly, noisy, and unhappy). Such insights are especially useful for marketing purposes in the travel industry, as well as the visualization city identities and the personification of cities.

Learning styles of photos. (Thomas and Kovashka 2016) introduce the novel problem of identifying the photographer behind a photograph. They created a dataset of over 180,000 images taken by 41 well-known photographers, and examined the effectiveness of a variety of features (low and high-level, including CNN features) at identifying the photographer. They also trained a deep convolutional neural network tailored for this task. They show that high-level features greatly outperform low-level features.

Learning vehicle style. Jae Lee et al. (2013) present a weakly-supervised visual data mining approach that discovers connections between recurring midlevel visual elements in historic (temporal) and geographic (spatial) image collections, and attempts to capture the underlying visual style. In contrast to existing discovery methods that mine for patterns that remain visually consistent throughout the dataset, they discover visual elements whose appearance changes due to change in time or location; i.e., exhibit consistent stylistic vari- ations across the label space (date or geo-location). Their approach first identify groups of patches that are style sensitive; it then incrementally builds correspondences to find the same element across the entire dataset. Finally, they train style-aware regressors that model each element’s range of stylistic differences. They apply it to date and geo-location prediction and show substantial improvement over several baselines that do not model visual style. The method’s effectiveness is also demonstrated on the related task of fine-grained classification.

Learning clothing style. Murillo, Kwak, Bourdev, Kriegman and Belongie (2012) consider photos of groups of people to learn which groups are more likely to socialize with one another. This implies learning a distance metric between images. However, they require manually specified styles, called “urban tribes”. Similarly, Bossard et al. (2012) who use a random forest approach to classify the style of clothing images, require pre-specified classes of style. Vittayakorn, Yamaguchi, Berg and Berg (2015) learn outfit similarity, based on specific descriptors for color, texture and shape. Therefore their system is able to retrieve similar outfits to a query image. McAuley, Targett, Shi and Van Den Hengel (2015) collect a large scale co-purchase dataset from Amazon and learn a notion of style and retrieve products from different categories that are supposed to be of similar style, using the image features from AlexNet (Krizhevsky et al. 2012) that was trained for object classification to learn their distance metric. 

Veit et al. (2015) address the exact same problem. Rather than using logistic regression, Veit et al. (2015) advance the method by fine-tuning the entire network with a Siamese architecture with a different sampling strategy, which they claim to be less prone to the cold-start problem in the previous paper. More specifically, Veit et al. (2015) propose a framework to learn a feature transformation from images of items into a latent space that expresses compatibility. For the feature transformation, a Siamese Convolutional Neural Network (CNN) architecture is used, where training examples are pairs of items that are either compatible or incompatible. The authors model compatibility based on large-scale user co-purchase data from Amazon, as do McAuley et al. (2015). Veit et al. (2015) construct pairs of heterogeneous dyads as training data in order to learn cross- category fit. They demonstrate that the proposed framework is capable of learning semantic information about visual style and is able to generate outfits of clothes, with items from different categories, that are esthetically compatible. 

Kiapour et al. (2014) further extend the identified style of clothing to personal wealth, occupation, and socio-identity, based on their assumption or observation that the clothing we wear and our identities are closed tied, revealing to the world clues other aspects of our lives. They designed an online competitive Style Rating Game called Hipster Wars to crowd source reliable human judgments of style, with which they collected a dataset of clothing outfits with associated style ratings for 5 style categories: hipster, bohemian, pinup, preppy, and goth. They train models for between- class and within-class classification of styles, and thus identifying clothing elements that are generally discriminative for a style, as well as items in a particular outfit that may indicate a style. Yamaguchi, Hadi Kiapour and Berg (2013) introduce an effective retrieval-based clothing parising method along with a large annotated dataset of fashion photos. For a query image, they find similar styles from a large database of tagged fashion images and use these examples to parse the query. Their approach combines parsing from: pre-trained global clothing models, local clothing models learned on the fly from retrieved examples, and transferred parse masks (paper doll item transfer) from retrieved examples. They show that this approach significantly outperforms state-of-the-art in parsing accuracy.

<h4> Fashion, Art, and Design </h4>
<h4> Fashion </h4>

Computer vision systems are designed to work well within the context of everyday photog- raphy. However, artists often render the world around them in ways that do not resemble photographs. Artwork produced by people is not constrained to mimic the physical world, making it more challenging for machines to recognize.
Chen, Xu, Liu and Zhu (2006) used an And-Or representation to build a tree of composite clothing templates by gathering and segmenting artists’ sketches and match those clothing template to the image. This is one of the first studies on clothing in the community.

Veit et al. (2015) propose a learning framework to recommend matching articles of cloth- ing to consumers. The type of questions their system is trained to answer are along the lines of “What outfit goes well with this pair of shoes?” The idea of this framework is to learn a feature transformation from images of items into a latent space that expresses com- patibility. For the feature transformation, a Siamese Convolutional Neural Network (CNN) architecture is used, where training examples are pairs of items that are either compatible or incompatible. The authors model compatibility based on large-scale user co-purchase data from Amazon. They construct pairs of heterogeneous dyads as training data in order to learn cross-category fit. They demonstrate that the proposed framework is capable of learning semantic information about visual style and is able to generate outfits of clothes, with items from different categories, that are esthetically compatible.

Vittayakorn et al. (2015) documents the first attempt to provide a quantitative analysis of
fashion on the runway and in the streets. They release a large-scale dataset of runway fashion photos representing 9, 328 fashion shows over 15 years, with which they develop a feature representation that can usefully capture the appearance of clothing items in outfits, through pose estimation, clothing parsing, and feature extraction. They collected human judgments of outfit similarity to train models for predicting similarity between runway outfits and street outfits. They found their proposed representation boosts prediction performance of the season, year, and brand, outperforming humans. [Nearest Neighbor, feature representation]

Bossard et al. (2012) build a pipeline for recognizing and classifying clothing in a natural setting, combining upper body detectors, style classification, attribute classification in a Random Forest.

Liu, Song, Liu, Xu, Lu and Yan (2012b) develop a system that takes a daily street snap- shot of individuals, and efficiently searches online for articles of clothing similar to the outfit in the street photo. It is a problem of cross-scenario clothing retrieval — the core of which lies in correctly identifying similarities between clothings against large discrepancies between street photos and online catalogue photos due to distinct human posture and environmental background. The proposed solution leverages human pose estimation and offline structure detection.

Chao et al. (2009) present a clothing recommendation system called the Smart Mirror. They use computer vision to recognize classes and attributes of clothing for personal fashion recommendation, mimicking a real-time customer fashion assistant in a store’s fitting room.


<h4> Style Transfer </h4>

The body of literature on style transfer grew rapidly since the seminal paper of Gatys et al. (2015), i.e., Gatys et al. (2016), first emerged in 2015.

Gatys et al. (2015) (or Gatys et al. (2016)) introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, the study offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.

In the aforementioned work, feed-forward convolutional neural networks are trained using a per-pixel loss between the output and ground-truth images. Meanwhile, parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. 

Johnson, Alahi and Fei-Fei (2016) propose to combine the benefits of both approaches, and demonstrate the use of perceptual loss functions for training feed-forward networks for image transformation tasks. They showcase how to solve the optimization problem underlying Gatys et al. (2015) in real-time. Compared to the optimization-based method, the network in Johnson et al. (2016) gives similar qualitative results but is three orders of magnitude faster. Extra experi- ments with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss also give rise to visually pleasing results. Local style transfer algorithms are based on spatial color mappings, more expressive, and can handle a broad class of applications such as painterly transfer (Selim, Elgharib and Doyle 2016, Li and Wand 2016). The approach, as is, is not suitable for photo realistic style transfer. Even when both the input and reference images are photographs, the output still exhibits distortions reminiscent of a painting. Luan, Paris, Shechtman and Bala (2017) introduce a deep-learning approach to photographic style transfer that handles a large variety of image content while faithfully transferring the ref- erence style. Their contribution is to constrain the transformation from the input to the output to be locally affine in color space, and to express this constraint as a custom fully differentiable energy term. They show that this approach successfully suppresses distortion and yields satisfying photo realistic style transfers in a broad variety of scenarios, including transfer of the time of day, weather, season, and artistic edits.


<h4> References (All Parts Combined) </h4>

Agarwal, Sameer, Noah Snavely, Ian Simon, Steven M Seitz, and Richard Szeliski, “Building rome in a day,” in “Computer Vision, 2009 IEEE 12th Interna- tional Conference on” IEEE 2009, pp. 72–79.

Azimi, Javad, Ruofei Zhang, Yang Zhou, Vidhya Navalpakkam, Jianchang Mao, and Xiaoli Fern, “Visual appearance of display ads and its effect on click through rate,” in “Proceedings of the 21st ACM international conference on Information and knowledge management” ACM 2012, pp. 495–504.

Berg, Thomas, Jiongxin Liu, Seung Woo Lee, Michelle L Alexander, David W Jacobs, and Peter N Belhumeur, “Birdsnap: Large-scale fine-grained visual cate- gorization of birds,” in “Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition” 2014, pp. 2011–2018.

Bossard, Lukas, Matthias Dantone, Christian Leistner, Christian Wengert, Till Quack, and Luc Van Gool, “Apparel classification with style,” in “Asian conference on computer vision” Springer 2012, pp. 321–335.

Bylinskii, Zoya, Nam Wook Kim, Peter O Donovan, Sami Alsheikh, Spandan Madan, Hanspeter Pfister, Fredo Durand, Bryan Russell, and Aaron Hertz- mann, “Learning Visual Importance for Graphic Designs and Data Visualizations,” in “Proceedings of the 30th Annual ACM Symposium on User Interface Software & Technology” 2017.

, Sami Alsheikh, Spandan Madan, Adria Recasens, Kimberli Zhong, Hanspeter Pfister, Fredo Durand, and Aude Oliva, “Understanding infographics through textual and visual tag prediction,” arXiv preprint arXiv:1709.09215, 2017.

Chan, Tian Heong, Ju ̈rgen Mihm, and Manuel E Sosa, “On Styles in Product Design: An Analysis of US Design Patents,” Management Science, 2017, 64 (3), 1230–1249.

Chandrasekaran, Arjun, Ashwin K Vijayakumar, Stanislaw Antol, Mohit Bansal, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh, “We are humor beings: Understanding and predicting visual humor,” in “Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition” 2016, pp. 4603–4612.

Chao, Xiaofei, Mark J Huiskes, Tommaso Gritti, and Calina Ciuhu, “A framework for robust feature selection for real-time fashion style recommendation,” in “Proceedings of the 1st international workshop on Interactive multimedia for consumer electronics” ACM 2009, pp. 35–42.

Chen, Hong, Zi Jian Xu, Zi Qiang Liu, and Song Chun Zhu, “Composite Templates for Cloth Modeling and Sketching,” in “Computer Vision and Pattern Recognition (CVPR), 2006 IEEE Conference on” IEEE 2006.

Cheng, Haibin, Roelof van Zwol, Javad Azimi, Eren Manavoglu, Ruofei Zhang, Yang Zhou, and Vidhya Navalpakkam, “Multimedia features for click prediction of new ads in display advertising,” in “Proceedings of the 18th ACM SIGKDD interna- tional conference on Knowledge discovery and data mining” ACM 2012, pp. 777–785.

Chilton, Lydia B, “Constructing Visual Metaphors for Creative Ads,” 2018.

, James A Landay, and Daniel S Weld, “HumorTools: A Microtask Workflow for Writing News Satire,” 2018.

Dew, Ryan, Asim Ansari, and Olivier Toubia, “Letting Logos Speak: A Machine Learning Approach for Data-Driven Logo Design,” 2018.

Doersch, Carl, Saurabh Singh, Abhinav Gupta, Josef Sivic, and Alexei A. Efros, “What Makes Paris Look like Paris?,” ACM Transactions on Graphics (SIGGRAPH), 2012, 31 (4), 101:1–101:9.

Dzyabura, Daria, Marat Ibragimov, and Siham El Kihal, 2018. Eliashberg, Jehoshua, Anita Elberse, and Mark AAM Leenders, “The motion picture industry: Critical issues in practice, current research, and new research directions,” Marketing science, 2006, 25 (6), 638–661.

Fleischman, Michael and Eduard Hovy, “Fine grained classification of named entities,” in “Proceedings of the 19th international conference on Computational linguistics- Volume 1” Association for Computational Linguistics 2002, pp. 1–7.

Frahm, Jan-Michael, Pierre Fite-Georgel, David Gallup, Tim Johnson, Rahul Raguram, Changchang Wu, Yi-Hung Jen, Enrique Dunn, Brian Clipp, Svetlana Lazebnik et al., “Building rome on a cloudless day,” in “European Conference on Computer Vision” Springer 2010, pp. 368–381.

Frome, Andrea, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Tomas Mikolov et al., “Devise: A deep visual-semantic embedding model,” in “Advances in neural information processing systems” 2013, pp. 2121–2129.

Garces, Elena, Aseem Agarwala, Diego Gutierrez, and Aaron Hertzmann, “A similarity measure for illustration style,” ACM Trans. Graph., 2014, 33, 93:1–93:9.

Gatys, Leon A, Alexander S Ecker, and Matthias Bethge, “A neural algorithm of artistic style,” arXiv preprint arXiv:1508.06576, 2015.

, , and , “Image style transfer using convolutional neural networks,” in “Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition” 2016, pp. 2414–2423.

Gebru, Timnit, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, and Li Fei-Fei, “Fine-Grained Car Detection for Visual Census Estimation.,” in “AAAI,” Vol. 2 2017, p. 6.

Horn, Grant Van and Pietro Perona, “The Devil is in the Tails: Fine-grained Classifi- cation in the Wild,” arXiv preprint arXiv:1709.01450, 2017.

Huang, Xinyue and Adriana Kovashka, “Inferring visual persuasion via body language, setting, and deep features,” in “Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops” 2016, pp. 73–79.

Hussain, Zaeem, Mingda Zhang, Xiaozhong Zhang, Keren Ye, Christopher Thomas, Zuha Agha, Nathan Ong, and Adriana Kovashka, “Automatic understanding of image and video advertisements,” in “2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)” IEEE 2017, pp. 1100–1110.

Johnson, Justin, Alexandre Alahi, and Li Fei-Fei, “Perceptual losses for real-time style transfer and super-resolution,” in “European Conference on Computer Vision” Springer 2016, pp. 694–711.

Joo, Jungseock, Francis F Steen, and Song-Chun Zhu, “Automated facial trait judg- ment and election outcome prediction: Social dimensions of face,” in “Proceedings of the IEEE international conference on computer vision” 2015, pp. 3712–3720.

, Weixin Li, Francis F Steen, and Song-Chun Zhu, “Visual persuasion: Inferring communicative intents of images,” in “Proceedings of the IEEE conference on computer vision and pattern recognition” 2014, pp. 216–223.

Karayev, Sergey, Matthew Trentacoste, Helen Han, Aseem Agarwala, Trevor Darrell, Aaron Hertzmann, and Holger Winnemoeller, “Recognizing image style,” arXiv preprint arXiv:1311.3715, 2013.

Karpathy, Andrej and Li Fei-Fei, “Deep visual-semantic alignments for generating image descriptions,” in “Proceedings of the IEEE conference on computer vision and pattern recognition” 2015, pp. 3128–3137.

Kiapour, M Hadi, Kota Yamaguchi, Alexander C Berg, and Tamara L Berg, “Hipster wars: Discovering elements of fashion styles,” in “European conference on computer vision” Springer 2014, pp. 472–488.

Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton, “ImageNet Classification with Deep Convolutional Neural Networks,” in F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, eds., Advances in Neural Information Processing Systems 25, Curran Associates, Inc., 2012, pp. 1097–1105.

Lee, Yong Jae, Alexei A. Efros, and Martial Hebert, “Style-Aware Mid-level Rep- resentation for Discovering Visual Connections in Space and Time,” in “The IEEE International Conference on Computer Vision (ICCV)” December 2013.

Li, Chuan and Michael Wand, “Combining markov random fields and convolutional neu- ral networks for image synthesis,” in “Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition” 2016, pp. 2479–2486.

Liu, Gaowen, Yan Yan, Elisa Ricci, Yi Yang, Yahong Han, Stefan Winkler, and Nicu Sebe, “Inferring Painting Style with Multi-Task Dictionary Learning.,” in “IJCAI” 2015, pp. 2162–2168.

Liu, Jiongxin, Angjoo Kanazawa, David Jacobs, and Peter Belhumeur, “Dog breed classification using part localization,” in “European Conference on Computer Vision” Springer 2012, pp. 172–185.

Liu, Liu and Dina Mayzlin, “Visual Listening in: Extracting Brand Image Portrayed on Social Media,” 2018.

Liu, Si, Zheng Song, Guangcan Liu, Changsheng Xu, Hanqing Lu, and Shuicheng Yan, “Street-to-shop: Cross-scenario clothing retrieval via parts alignment and auxil- iary set,” in “Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Confer- ence on” IEEE 2012, pp. 3330–3337.

Liu, Xiao, Dokyun Lee, and Kannan Srinivasan, “Large Scale Cross Category Analysis of Consumer Review Content on Sales Conversion Leveraging Deep Learning,” 2017.

Lu, Shasha, Li Xiao, and Min Ding, “A video-based automated recommender (VAR) system for garments,” Marketing Science, 2016, 35 (3), 484–510.

Luan, Fujun, Sylvain Paris, Eli Shechtman, and Kavita Bala, “Deep Photo Style Transfer,” arXiv preprint arXiv:1703.07511, 2017.

Maji, Subhransu, Esa Rahtu, Juho Kannala, Matthew Blaschko, and An- drea Vedaldi, “Fine-grained visual classification of aircraft,” arXiv preprint arXiv:1306.5151, 2013.

Malik, Nikhil, Param Vir Singh, Dokyun Lee, and Kannan Srinivasan, “When Does Beauty Pay. A Large Scale Image Based Appearance Analysis on Career Transitions,” 2018.

McAuley, Julian, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel, “Image-based recommendations on styles and substitutes,” in “Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval” ACM 2015, pp. 43–52.

McDuff, Daniel, Rana El Kaliouby, Jeffrey F Cohn, and Rosalind W Picard, “Predicting ad liking and purchase intent: Large-scale analysis of facial responses to ads,” IEEE Transactions on Affective Computing, 2015, 6 (3), 223–235.

Mei, Tao, Lusong Li, Xian-Sheng Hua, and Shipeng Li, “ImageSense: Towards contextual image advertising,” ACM Transactions on Multimedia Computing, Commu- nications, and Applications (TOMM), 2012, 8 (1), 6.

Murillo, Ana C, Iljung S Kwak, Lubomir Bourdev, David Kriegman, and Serge Belongie, “Urban tribes: Analyzing group photos from a social perspective,” in “Com- puter Vision and Pattern Recognition Workshops (CVPRW), 2012 IEEE Computer Society Conference on” IEEE 2012, pp. 28–35.

Papatla, Purushottam, “Face, Body or Both? Effects of Partial and Full Visibility of People in VUGC on Consumer Response,” 2018.

Quercia, Daniele, Neil Keith O’Hare, and Henriette Cramer, “Aesthetic capital: what makes london look beautiful, quiet, and happy?,” in “CSCW” 2014.

Rohrbach, Anna, Marcus Rohrbach, Niket Tandon, and Bernt Schiele, “A dataset for movie description,” in “Proceedings of the IEEE conference on computer vision and pattern recognition” 2015, pp. 3202–3212.

Rohrbach, Marcus, Sikandar Amin, Mykhaylo Andriluka, and Bernt Schiele, “A database for fine grained activity detection of cooking activities,” in “Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on” IEEE 2012, pp. 1194– 1201.

S ́anchez, Juan M, Xavier Binefa, and Jordi Vitria, “Shot partitioning based recognition of tv commercials,” Multimedia Tools and Applications, 2002, 18 (3), 233–247.

Selim, Ahmed, Mohamed Elgharib, and Linda Doyle, “Painting style transfer for head portraits using convolutional neural networks,” ACM Transactions on Graphics (ToG), 2016, 35 (4), 129.

Shi, Zijun (June), Dokyun Lee, Param Vir Singh, and Kannan Srinivasan, “Design of Fashion: Can Brand Value be Separated from Style Value?,” 2018.

Snavely, Noah, Steven M Seitz, and Richard Szeliski, “Photo tourism: exploring photo collections in 3D,” in “ACM transactions on graphics (TOG),” Vol. 25 ACM 2006, pp. 835–846.

, , and , “Modeling the world from internet photo collections,” International journal of computer vision, 2008, 80 (2), 189–210.

Tapaswi, Makarand, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja Fidler, “Movieqa: Understanding stories in movies through question-answering,” in “Proceedings of the IEEE conference on computer vision and pattern recognition” 2016, pp. 4631–4640.

Thomas, Christopher and Adriana Kovashka, “Seeing behind the camera: Identifying the authorship of a photograph,” in “Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition” 2016, pp. 3494–3502.

Tkachenko, Yegor, Asim Ansari, and Olivier Toubia, “Computer-aided Exploration Of Product Designs in High-dimensional Visual Spaces,” 2018.

Todorov, Alexander, “Modeling Visual Impressions of Faces,” 2018.

Veit, Andreas, Balazs Kovacs, Sean Bell, Julian McAuley, Kavita Bala, and Serge Belongie, “Learning visual clothing style with heterogeneous dyadic co-occurrences,” in “Proceedings of the IEEE International Conference on Computer Vision” 2015, pp. 4642–4650.

Vittayakorn, Sirion, Kota Yamaguchi, Alexander C Berg, and Tamara L Berg, “Runway to realway: Visual analysis of fashion,” in “Applications of Computer Vision (WACV), 2015 IEEE Winter Conference on” IEEE 2015, pp. 951–958.

Wilber, Michael J, Chen Fang, Hailin Jin, Aaron Hertzmann, John Collomosse, and Serge J Belongie, “BAM! The Behance Artistic Media Dataset for Recognition Beyond Photography.,” in “ICCV” 2017, pp. 1211–1220.

Xiao, Jianxiong and Yasutaka Furukawa, “Reconstructing the worlds museums,” In- ternational journal of computer vision, 2014, 110 (3), 243–258.

Xiao, Li and Min Ding, “Just the Faces: Exploring the Effects of Facial Features in Print Advertising,” Marketing Science, 2014, 33, 338–352.

Yadati, Karthik, Harish Katti, and Mohan Kankanhalli, “CAVVA: Computational affective video-in-video advertising,” IEEE Transactions on Multimedia, 2014, 16 (1), 15–23.

Yamaguchi, Kota, M Hadi Kiapour, and Tamara L Berg, “Paper doll parsing: Re- trieving similar styles to parse clothing items,” in “Proceedings of the IEEE interna- tional conference on computer vision” 2013, pp. 3519–3526.

, M Hadi Kiapour, Luis E Ortiz, and Tamara L Berg, “Parsing clothing in fashion photographs,” in “Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on” IEEE 2012, pp. 3570–3577.

Yang, Linjie, Ping Luo, Chen Change Loy, and Xiaoou Tang, “A large-scale car dataset for fine-grained categorization and verification,” in “Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition” 2015, pp. 3973–3981.

Yang, Yi and Deva Ramanan, “Articulated pose estimation with flexible mixtures-of- parts,” in “Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on” IEEE 2011, pp. 1385–1392.

Yoganarasimhan, Hema, “Identifying the presence and cause of fashion cycles in data,” Journal of Marketing Research, 2017, 54 (1), 5–26.

Zhang, Shunyuan, Dokyun Lee, Param Vir Singh, and Kannan Srinivasan, “How Much is an Image Worth? The Impact of Professional versus Amateur Airbnb Property Images on Property Demand,” 2018.

Zhao, Gangqiang, Junsong Yuan, Jiang Xu, and Ying Wu, “Discovering the thematic object in commercial videos,” IEEE MultiMedia, 2011, 18 (3), 56–65.

Zhou, YingHui, Shasha Lu, and Min Ding, “A Face Anonymity-Perceptibility Paradigm and an Application in the Online Dating Industry,” 2016.

Zou, Chuhang, Alex Colburn, Qi Shan, and Derek Hoiem, “LayoutNet: Recon- structing the 3D Room Layout from a Single RGB Image,” in “Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition” 2018, pp. 2051–2059.
