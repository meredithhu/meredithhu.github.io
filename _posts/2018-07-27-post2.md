---
layout: post
title: A Survey: Marketing and Computer Vision, Part I
date: 2017-07-16
---

Image Processing or Computer Vision in Quantitative Marketing
                                                                                         
<html>                                                                                      
</a>Product Design</h4>
<a href="#Xdaria2018return">Dzyabura, Ibragimov and Kihal</a>&#x00A0;(<span class="bib-year"><a 
href="#Xdaria2018return">2018</a></span>) use machine learning models to predict demand in
online and offline retail channels, as well as returns for new products. They measure sale
distributions across channels for each product category and find those predominantly
sold online are more prone to customer returns. They also demonstrate the superior
prediction performance when product image features &#8212; color histograms, texture, learned
representation by training AlexNet (<a 
href="#XNIPS2012_4824">Krizhevsky, Sutskever and Hinton</a>&#x00A0;<a 
href="#XNIPS2012_4824">2012</a>) &#8212; are
included.
 <a href="#Xzhanglee2018">Zhang, Lee, Singh and Srinivasan</a>&#x00A0;(<span class="bib-year"><a 
href="#Xzhanglee2018">2018</a></span>) estimate the economic impact of images
and low-level image features that property demand in Airbnb. By classifying property
photos with computer vision and deep learning models, they show that 48<span 
class="cmmi-12">.</span>9% of the
effect of verified images boosting demand comes from the high image quality. They
also identify 12 image attributes based on marketing and photography literature and
demonstrate the direct impacts of these on demand after controlling for many obserables, thus
prescribing optimal product image strategies to increase demand for housing and lodging
managers.
<a href="#Xtoubia3">Tkachenko, Ansari and Toubia</a>&#x00A0;(<span class="bib-year"><a 
href="#Xtoubia3">2018</a></span>) apply deep learning techniques for the purpose of
computer-aided exploration of visual product designs, where alternative design methods, such as
conjoint or brute-force search, may not be applicable or may perform suboptimally. In this work,
they first confirm properties of the lower-dimensional latent space that are desirable &#8212; they show
(a) how distance between images in this latent space mimics human similarity judgments about the
actual images, and (b) that important characteristics of interest, such as product prices, can be
predicted from latent image data alone. Building on these findings, they propose and
demonstrate machine learning techniques for exploration and ideation in the design space, such
as image interpolation to generate product designs that are similar to competitorâs
products, or constrained Bayesian optimization to find novel designs that score high on
quantitative characteristics of interest. They use images and attributes of products sold
on Amazon.com as well as human feedback from Amazon Turk workers as a basis for
experiments. Their results imply that deep generative models offer a promising avenue for
partial automation of the visual product design process. This is perhaps the first study in
marketing that has leveraged the GANs that have been all the rage in the machine
learning community. To its core, it is an application of generative models and visual embeddings to the domain of product design. Without any new dataset, or new theories as
the backbone, it would appear to be but another application of the aforementioned
methods, since similar studies in computer vision (and natural language processing)
abound. We review the relevant computer vision literature in the next section (Part II);
<a href="#x1-160003.6.2"></a>.
<a href="#Xchan2017styles">Chan, Mihm and Sosa</a>&#x00A0;(<span class="bib-year"><a 
href="#Xchan2017styles">2017</a></span>) look into the ebbs and flows of styles in product design and how
it relates to the evolution of product functionality over time, based on a large-scale US
design patent dataset. They identify the styles in design using clustering and provide
empirical evidence for the long-lived architectural and design mantra &#8220;Form follows
function&#8221;.
<!--l. 248--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">2.2   </span> <a 
 id="x1-40002.2"></a>Branding, and Visual User-Generated Content</h4>
<!--l. 250--><p class="noindent" >
 <a href="#Xliuliu2018">Liu and Mayzlin</a> propose a &#8220;visual listening in&#8221; approach to measuring how brands are
portrayed on social media (Instagram) by mining visual content posted by users. They use
supervised machine learning methods, traditional support vector machine classifiers and deep
convolutional neural networks, to measure brand attributes (glamorous, rugged, healthy, fun) from
images. Then they apply the classifiers to brand-related images posted on social media to
measure what consumers are visually communicating about brands. By comparing the
portrayals of 56 brands in the apparel and beverages categories in consumer-created
images with images on the firmâs official Instagram account, as well as with consumer
brand perceptions measured in a national brand survey, they find, despite convergent
validity shown in all three measures, key differences between how consumers and firms
portray the brands on visual social media, and how the average consumer perceives the
brands.
<!--l. 266-->
<a href="#Xlogodew2018">Dew, Ansari and Toubia</a>&#x00A0;(<span class="bib-year"><a 
href="#Xlogodew2018">2018</a></span>) explore the visual elements in logos that express brand
personality traits, based on which they introduce a logo tokenization algorithm that
decomposes logos into theory-base and human-meaningful visual features. Applied to
a small dataset of logos, matched with textual data from firms&#8217; websites, consumer
evaluations of brands, third-party descriptions of the companies, they uncover uncovers links
that exist between a brandâs logo, description, and personality, and thereby facilitate a
better understanding of the underpinnings of good design, and inform the design of new logos.
<!--l. 279--><p class="indent" >   Photos posted on social media often include people using specific brands of products. Online
retailers sometimes display the photos on their sites hoping for positive brand image and
observational learning. However sometimes, it is the person in the photo, rather than the brand,
that attracts the visitors&#8217; attention. <a 
href="#Xpapatla2018">Papatla</a>&#x00A0;(<span class="bib-year"><a 
href="#Xpapatla2018">2018</a></span>) investigates whether the presence of faces in
VUGC could be less detrimental if they are less prominent, based on findings that faces and bodies
of humans in the visual field are processed holistically even if they are seen as distinct stimuli.
They analyze consumer response to about 12,000 photos of 800 different products in six categories
displayed by 35 online retailers.
<!--l. 291--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">2.3   </span> <a 
 id="x1-50002.3"></a>Fashion and Beauty</h4>
<!--l. 293--><p class="noindent" ><span 
class="cmbx-12">? </span>study the substitutability between the brand value and the style value in the fashion market.
They quantify the style value by employing deep learning based computer vision techniques to
create style features, including clothing style (e.g., compatibility between clothing items,
creativity), model style (e.g., facial and body attractiveness), and photo style. These style features
are incorporated in a dynamic structural model to estimate a dynamic structural model to analyze
the content creation and consumption behavior of influencers in a fashion social network
community. They find significant effects of brands and style features on the trendiness of a fashion
look, as well as substitutability patterns between style features and brand levels. For managers and
influencers in the fashion market, their results provide guidelines on how to engineer
a fashion &#8220;look&#8221; that can attract the most attention. This is a nice addition to the
economic paradigm of feature substitutability analyses, bolstered by computer vision
methods.
<!--l. 310--><p class="indent" >   <a 
href="#Xbeautylee2018">Malik, Singh, Lee and Srinivasan</a>&#x00A0;(<span class="bib-year"><a 
href="#Xbeautylee2018">2018</a></span>) investigate the the dynamic effects of beauty over an
individualâs career. They use computer vision methods to score the attractiveness of
each individual in a longitudinal sample on career milestones, with which they estimate
a survival analysis where attractive men are found to progress faster in their career
early on and women are found to progress faster in their later career in comparison to
their unattractive counterparts respectively. In other words, they find that men enjoy
a beauty premium early in their career which disappears later in the career, whereas
the opposite goes for women, even though the overall beauty premium is greater for women.
<!--l. 324--><p class="indent" >   <a 
href="#Xtodorov2018">Todorov</a>&#x00A0;(<span class="bib-year"><a 
href="#Xtodorov2018">2018</a></span>) model social perception of faces using data-driven approaches whose objective
is to identify quantitative relationships between high-dimensional variables (e.g., visual images)
and behaviors (e.g., perceptual decisions) with as little bias as possible. They conduct a series of
studies using reverse correlation methods based on judgments of randomly generated faces
from a statistical, multidimensional face model; a vector space where every face can be
represented as a vector in the space. These methods are used to a) model evaluation of
faces on any social dimension (e.g., trustworthiness), and b) to identify the perceptual
basis of this evaluation, thus mapping configurations of face features to specific social
inferences.
<!--l. 336--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">2.4   </span> <a 
 id="x1-60002.4"></a>Advertising</h4>
<!--l. 338--><p class="noindent" ><a 
href="#XXiao2014JustTF">Xiao and Ding</a>&#x00A0;(<span class="bib-year"><a 
href="#XXiao2014JustTF">2014</a></span>) study the effect of non-celebrity faces in print advertising with established
face recognition methods from computer vision. Specifically, they propose the use of eigenface
features to segment people based on their preferences towards different faces. Siginificant
and substantial effects of faces on viewers&#8217; attitudes towards the ad, the brand, and
their purchase intentions were documented. Considerable consistency is identified within
subject, whereas substantial heterogeneity exists between subjects and among product
categories: certain eigenfaces are more predictive of greater viewer affinity for certain
product categories. They also find that the effect of faces interacts with product categories
and is mediated by various facial traits such as attractiveness, trustworthiness, and
competence.
<!--l. 358--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">2.5   </span> <a 
 id="x1-70002.5"></a>Applications of Face Recognition</h4>
<a href="#Xlu2016video">Lu, Xiao and Ding</a>&#x00A0;(<span class="bib-year"><a 
href="#Xlu2016video">2016</a></span>) claim to have developed one of the first attempts to integrate video
analysis &#8212; real time facial expression recognition and hand detection &#8212; at the individual
customer level with extant marketing research methods to create useful managerial
tools in retail. They created an automatic and scalable garment recommender system
that:
<!--l. 367--><p class="indent" >
<ol  class="enumerate1" >
      <li class="enumerate" id="x1-7002x1">uses a camera to capture consumer behavior in front of the mirror to make inferences about her preferences based on recognized facial expressions and region of interest
      (clothing being tried on);
      </li>
      <li class="enumerate" id="x1-7004x2">matches the customer with a database of individuals with known fashion tastes and
      preferences, within which those with similar tastes are identified as nearest neighbors;
      </li>
      <li class="enumerate" id="x1-7006x3">makes fashion recommendations to the focal customer based on preferences of identified individuals in the database.</li></ol>
<!--l. 377--><p class="noindent" >A similar system called the Smart Mirror (
 <a href="#Xchao2009framework">Chao, Huiskes, Gritti and Ciuhu</a>&#x00A0;
 <a href="#Xchao2009framework">2009</a>) predated theirs
seven years prior. The only difference between this marketing paper and the previous Smart
Mirror is that the marketing authors added another feature of facial recognition to
estimate customers&#8217; emotional responses to whatever clothing they are trying on. Such
an extra design element could backfire due to the heterogeneity of individual facial
expressions.
<!--l. 387--><p class="indent" ><a href="#Xzhou2016face">Zhou, Lu and Ding</a>&#x00A0;(<span class="bib-year"><a 
href="#Xzhou2016face">2016</a></span>) propose a computer vision based framework to anonymize and
distinguish facial images in online dating, which they, supposedly innocent non-English-speakers,
termed as a FAP paradigm &#8212; a face anonymity-perceptibility framework that brings together face
anonymity research from computer vision literature, and the perceptibility studies from the social
and the neuropsychology literature for marketing applications such as online dating, hiring, sales,
and security. They select a set of facial landmarks for local or global facial features depending on
the abstraction method used to anonymize. Then they show users abbreviated profiles including
facial abstractions that preserve anonymity and perceptibility at the same time for preference
estimation. Finally, a smaller set of potential partners are selected for the user to the best of his or
her liking.




<!--l. 407--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">3.   </span> <a 
 id="x1-80003"></a>Marketing Applications of Computer Vision</h3>                                                                                    
<h4 class="subsectionHead"><span class="titlemark"> <a id="x1-90003.1"></a>Advertising</h4>
<!--l. 416--><p class="noindent" ><a 
href="#Xjoo2014visual">Joo, Li, Steen and Zhu</a>&#x00A0;(<span class="bib-year"><a 
href="#Xjoo2014visual">2014</a></span>) first introduce the problem of understanding visual persuasion in
computer vision. A persuasive image has an underlying intention to persuade the viewer by its
visuals and is widely used in mass media, such as TV news, advertisements, and political
campaigns. <a 
href="#Xjoo2014visual">Joo et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xjoo2014visual">2014</a></span>) focus on understanding the underlying intents of such persuasive
images, for which they collected a a dataset of 1<span 
class="cmmi-12">, </span>124 images of politicians labeled with
ground-truth intents by ranking. They identify twelve syntactical features as predictors and nine
dimensions of communicative intents as labels. The communicative intents are grouped into three
buckets:
      <ol  class="enumerate1" >
      <li class="enumerate" id="x1-9002x1">Emotional Traits: Happy, Angry, Fearful;
      </li>
      <li class="enumerate" id="x1-9004x2">Personality  Traits  and  Values:  Competent,  Energetic,  Comforting,  Trustworthy,
      Socially Dominant;
      </li>
      <li class="enumerate" id="x1-9006x3">Overall Favorability: Favorable.</li></ol>
<!--l. 432--><p class="noindent" >As are syntactical features:
      <ol  class="enumerate1" >
      <li class="enumerate" id="x1-9008x1">Facial Display: Smile, Look Down, Eye Open, Mouth Open;
      </li>
      <li class="enumerate" id="x1-9010x2">Body Cues - Gestures: Hand-Wave, Hand-Shake, Finger-point, Touch-Head, Hug;
      </li>
      <li class="enumerate" id="x1-9012x3">Scene Context: Large Crowd, Dark-Background, Indoor.</li></ol>
<!--l. 439--><p class="noindent" >Figure 6 from the original study is reproduced in Figure&#x00A0;<span 
class="cmbx-12">??</span>. For all dimensions, the full
approach that exploits all three types of syntactical features yields the best result. In
addition, the facial display type outperforms the other cues on the emotional dimensions
while the gesture type is more discriminative for 3 among 5 dimensions of personality traits
and values. <hr class="figure"><div class="figure" 
><a 
 id="x1-9013r1"></a> <img 
src="0_Users_sheng_Dropbox_AAA_survey_images_fig6.png" alt="PIC"  
>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;1: </span><span  
class="content">Figure 6 in <a 
href="#Xjoo2014visual">Joo et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xjoo2014visual">2014</a></span>): Intents prediction performance evluation</span></div><!--tex4ht:label?: x1-9013r1 -->
<!--l. 451--><p class="indent" >   </div><hr class="endfigure">
                                                                                         
                                                                                         
<!--l. 453--><p class="indent" >   <a 
href="#Xhussain2017automatic">Hussain, Zhang, Zhang, Ye, Thomas, Agha, Ong and Kovashka</a>&#x00A0;(<span class="bib-year"><a 
href="#Xhussain2017automatic">2017</a></span>) create two
datasets: an image dataset of 64,832 image ads and a video dataset of 3,477 ads, both of
which contain rich annotations of the topics, the sentiments, questions and answers
describing the objectives, the reasoning the ad presents to persuade the viewer, as well
as the symbolic references ads make. The authors develop computer vision system to
understand these ads and evaluate it on tasks such as symbolic question-answering, topic and
sentiment recognition. The highest accuracy they achieved on symbolism prediction is 50%
on the image dataset, and the accuracies for topic and sentiment predictions on video
ads were 35<span 
class="cmmi-12">.</span>1% and 32<span 
class="cmmi-12">.</span>8%, respectively. They also predicted whether the videos ads
were funny or exciting &#8212; the resulting accuracies were 78<span 
class="cmmi-12">.</span>6% and 78<span 
class="cmmi-12">.</span>2% accordingly.
More interesting applications could build on the result from <a 
href="#Xhussain2017automatic">Hussain et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xhussain2017automatic">2017</a></span>). For
instance,
      <ul class="itemize1">
      <li class="itemize">methods could be developed to predict how effective a certain ad will be given a target
      audience;
      </li>
      <li class="itemize">on the part of viewers, the automatic ad-understanding system could help prevent
      being tricked into buying certain products;
      </li>
      <li class="itemize">by  decoding  the  messages  of  ads,  better  ad-targeting  strategies  could  be  tailored
      according to user interests;
      </li>
      <li class="itemize">it could be extended to generate automatic ad descriptions and summarizations.</li></ul>
<!--l. 476--><p class="noindent" ><a 
href="#Xazimi2012visual">Azimi, Zhang, Zhou, Navalpakkam, Mao and Fern</a>&#x00A0;(<span class="bib-year"><a 
href="#Xazimi2012visual">2012</a></span>) study the relationship between the visual
appearance and performance of creatives using large scale data in the worldâs (then) largest
display ads exchange system, RightMedia. They design a set of 43 visual features, categorized into
three different sets:
      <ul class="itemize1">
      <li class="itemize">Global  Features:  Grey  Level  Features,  Color  Distributions,  Model-Based  Color
      Harmony, Color Coherence, Hue Distribution, Lightness Features;
                                                                                         
                                                                                         </li>
      <li class="itemize">Local  Features:  Segment  Size,  Segment  Hues,  Segment  Color  Harmony,  Segment
      Lightness;
      </li>
      <li class="itemize">Advanced Features: Saliency Features, Number of Characters, Number of Faces.</li></ul>
<!--l. 489--><p class="noindent" >They extract the visual features and conduct a series of experiments to evaluate the effectiveness of
visual features for click through rate prediction, ranking and performance classification. Based on
the evaluation results, they selected a subset of features that have the most important
impact on click through rates, useful for ads selection and developing visually appealing
creatives.
<!--l. 496--><p class="indent" >   State-of-the-art prediction algorithms for click-through rates on non-guaranteed
display advertising rely heavily on historical information collected for advertisers, users
and publishers, which makes it challenging when new advertisements are encountered
due to the lack of historical data. <a 
href="#Xcheng2012multimedia">Cheng, Zwol, Azimi, Manavoglu, Zhang, Zhou and
Navalpakkam</a>&#x00A0;(<span class="bib-year"><a 
href="#Xcheng2012multimedia">2012</a></span>) propose to mitigate this problem by integrating multimedia features
extracted from display ads into the click prediction models because multimedia features can help
capture the attractiveness of the ads with similar contents or aesthetics. The authors
evaluate the use of numerous multimedia features, in addition to commonly used user,
advertiser and publisher features, and demonstrate that adding multimedia features can
significantly improve the accuracy of click prediction for new ads, compared to a baseline
model.
<!--l. 510--><p class="indent" >   <a 
href="#Xchilton2018">Chilton</a>&#x00A0;(<span class="bib-year"><a 
href="#Xchilton2018">2018</a></span>) tackle the problem of creative ad generation given a central message to be
conveyed to the target audience. For instance, given the message &#8220;Smoking kills you&#8221;, their system
outputs an image that blends two visual symbols &#8212; one represents the subject (&#8220;smoking&#8221;) and
one represents the predicate (&#8220;kills you&#8221;) &#8212; a handgun loaded with cigarettes. More details to be
found in their draft yet to be released.
<!--l. 518--><p class="indent" >   <a 
href="#Xmcduff2015predicting">McDuff, El&#x00A0;Kaliouby, Cohn and Picard</a>&#x00A0;(<span class="bib-year"><a 
href="#Xmcduff2015predicting">2015</a></span>) collected over 12<span 
class="cmmi-12">, </span>0000 facial responses from
1<span 
class="cmmi-12">, </span>223 people to 170 ads from a range of markets and product categories. The facial responses were
automatically coded frame by frame at a large scale. Detected expressions were found sparse but
aggregate responses revealed rich emotion trajectories. Ad liking were predicted accurately
from webcam facial responses at a ROC AUC at 0<span 
class="cmmi-12">.</span>85, followed by a change in purchase
intent at a ROC AUC at 0<span 
class="cmmi-12">.</span>78. They found that ad liking is driven by mostly positive
                                                                                         
                                                                                         
expressions, whereas determinants of purchase intent are more complex: peak positive
responses that are immediately preceded by a brand appearance are more likely to be
effective.
<!--l. 530--><p class="indent" >   <a 
href="#Xmei2012imagesense">Mei, Li, Hua and Li</a>&#x00A0;(<span class="bib-year"><a 
href="#Xmei2012imagesense">2012</a></span>) presents a contextual advertising system driven by images, which
automatically associates relevant ads with an image rather than the entire text in a Web page
and seamlessly inserts the ads in the non-intrusive areas within each individual image.
The proposed system, called ImageSense, supports scalable advertising of, from root
to node, Web sites, pages, and images. In ImageSense, the ads are selected based on
not only textual relevance but also visual similarity, so that the ads yield contextual
relevance to both the text in the Web page and the image content. The ad insertion
positions are detected based on image salience, as well as face and text detection, to
minimize intrusiveness to the user. They collected a unique advertisement-image dataset
that consists of 7<span 
class="cmmi-12">, </span>285 unique ad product logos with annotations of 32<span 
class="cmmi-12">, </span>480 unique ad
words done by 20 subjects, as well as 382<span 
class="cmmi-12">, </span>371 images from <a 
href="http://www.tango.msra" class="url" ><span 
class="cmtt-12">http://www.tango.msra</span></a> and
200<span 
class="cmmi-12">, </span>000 images from <a 
href="http://www.flickr.com" >Flickr</a>, based on which they evaluate ImageSense with 1<span 
class="cmmi-12">, </span>100 ad
triggering pages (100 web pages from major news sites, 1<span 
class="cmmi-12">, </span>000 images searched by top
100 image queries), and demonstrate the effectiveness of ImageSense for online image
advertising.
<!--l. 559--><p class="indent" >   <a 
href="#Xyadati2014cavva">Yadati, Katti and Kankanhalli</a>&#x00A0;(<span class="bib-year"><a 
href="#Xyadati2014cavva">2014</a></span>) propose an in-stream video advertising strategy for
advertising platforms such as <a 
href="http://www.youtube.com" >Youtube</a>, which they term as Computational Affective
Video-in-Video Advertising (CAVVA). They focus on the role emotions play in influencing the
buying behavior of users and therefore factor in the emotional impact of the videos as well as
advertisements. Given a video and a set of advertisements, their method first identify candidate
advertisement insertion points, as well as the suitable advertisements based on marketing and
consumer psychology theories. The two stage problem is then integrated as a single optimization
function in a non-linear 0â1 integer programming framework and a solution was identified based on
genetic algorithm. They evaluate CAVVA using a subjective user-study and eye-tracking
experiment. They are able to demonstrate that CAVVA strikes a balance between the
conflicting goals of (a) minimizing the user disturbance because of advertisement insertion
while (b) enhancing the user engagement with the advertising content. Their method is
pitted against existing advertising strategies and shown that CAVVA can enhance the
userâs experience and also help increase the monetization potential of the advertising
content.
                                                                                         
                                                                                         
<!--l. 579--><p class="indent" >   <a 
href="#Xsanchez2002shot">Sánchez, Binefa and Vitrià</a>&#x00A0;(<span class="bib-year"><a 
href="#Xsanchez2002shot">2002</a></span>) approach the problem of automatic TV commercials
recognition, and introduce an algorithm for scene break detection. The structure of each
commercial is represented by the set of its key-frames, which areautomatically extracted from the
video stream. The commonly used shot boundary detection techniques are based on individual
image features or visual cues, which show significant performance lacks when they are applied to
complex video content domains like commercials. The authors present a scene break detection
algorithm based on the combined analysis of edge and color features. Local motion estimation is
applied to each edge in a frame, and the continuity of the color around them is then
checked in the following frame and thus the continuous presence of the objects and/or the
background of the scene during each shot is essential for the proposed algorithm. They
show that this approach outperforms single feature algorithms in terms of precision and
recall.
<!--l. 595--><p class="indent" >   <a 
href="#Xzhao2011discovering">Zhao, Yuan, Xu and Wu</a>&#x00A0;(<span class="bib-year"><a 
href="#Xzhao2011discovering">2011</a></span>) propose a data-mining method for thematic object discovery in
commercials by finding spatially collocated visual features, such as logos, human facial reactions,
ad placement and recognition, and detecting logos, are quite distinct from our goal of decoding the
messages of ads.
<!--l. 601--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">3.2   </span> <a 
 id="x1-100003.2"></a>Storytelling</h4>
<!--l. 605--><p class="noindent" ><a 
href="#Xrohrbach2015dataset">Rohrbach, Rohrbach, Tandon and Schiele</a>&#x00A0;(<span class="bib-year"><a 
href="#Xrohrbach2015dataset">2015</a></span>) introduce a dataset of transcribed Descriptive
video service (DVS) that is temporally aligned to full length HD movies. DVS provides linguistic
descriptions of movies and are by design mainly visual. In addition, the aligned movie scripts were
also collected for comparison between the two different sources of descriptions. In total the Movie
Description dataset contains a parallel corpus of over 54,000 sentences and video snippets from 72
HD movies. Comparing DVS to scripts, they find that DVS is far more visual and describes
precisely what is shown rather than what should happen according to the scripts created prior to
movie production. Building on <a 
href="#Xrohrbach2015dataset">Rohrbach et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xrohrbach2015dataset">2015</a></span>), <a 
href="#Xtapaswi2016movieqa">Tapaswi, Zhu, Stiefelhagen,
Torralba, Urtasun and Fidler</a>&#x00A0;(<span class="bib-year"><a 
href="#Xtapaswi2016movieqa">2016</a></span>) introduce the MovieQA dataset which aims to
evaluate automatic story comprehension from both video and text. The dataset consists of
14,944 questions about 408 movies with high semantic diversity. The questions range
from simpler &#8220;Who&#8221; did &#8220;What&#8221; to &#8220;Whom&#8221;, to &#8220;Why&#8221; and &#8220;How&#8221; certain events
occurred. Each question comes with a set of five possible answers; a correct one and four
                                                                                         
                                                                                         
deceiving answers provided by human annotators. Multiple sources of information â
video clips, plots, subtitles, scripts, and DVS (<a 
href="#Xrohrbach2015dataset">Rohrbach et al.</a>&#x00A0;<a 
href="#Xrohrbach2015dataset">2015</a>) were all included.
The authors extended existing QA techniques to show that question-answering with
such open-ended semantics is hard and much future work awaits in this challenging
domain.
<!--l. 628--><p class="indent" >   Humor is a highly-valued human skill &#8212; arguably a sign of intelligence and creativity. Humor
creation is a long-standing problem in Artificial Intelligence, because it does not easily decompose
and it cannot readily be defined or detected. <a 
href="#Xchandrasekaran2016we">Chandrasekaran, Vijayakumar, Antol, Bansal, Batra,
Lawrence&#x00A0;Zitnick and Parikh</a>&#x00A0;(<span class="bib-year"><a 
href="#Xchandrasekaran2016we">2016</a></span>) document progress in understanding subtleties of human
expressions such as humor. They collected two datasets of abstract scenes that facilitate
the study of humor at both the scene-level and the object-level. They annotated the
funny scenes and explore the different types of humor depicted in them. By designing
computational models that predict the funniness and alter the funniness of a scene, they were
able to provide answers to questions such as &#8220;what content in a scene causes it to be
funny?&#8221; They show that their models perform well quantitatively, and qualitatively
through human studies. In the same vein, <a 
href="#Xchiltonhumortools">Chilton, Landay and Weld</a>&#x00A0;(<span class="bib-year"><a 
href="#Xchiltonhumortools">2018</a></span>) surveyed
professional comedians and found evidence that the humor-generation process can be
described. Based on this survey, they performed an analysis of news satire from <span 
class="cmti-12">The</span>
<span 
class="cmti-12">Onion </span>and decomposed the process of humor creation into seven microtasks &#8212; aspect,
expected reactions, expected reasons, associations, expectations violation mechanisms,
beliefs, and evaluation. They then developed a workflow that invokes these microtasks
dynamically. They validated and evaluated the microtasks and workflow with 20 human
subjects.
<!--l. 650--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">3.3   </span> <a 
 id="x1-110003.3"></a>Face Detection</h4>
<!--l. 652--><p class="noindent" ><a 
href="#Xjoo2015automated">Joo, Steen and Zhu</a>&#x00A0;(<span class="bib-year"><a 
href="#Xjoo2015automated">2015</a></span>) design a fully automated system that can infer the perceived traits of a
person from his face &#8212; social dimensions, such as &#8220;intelligence&#8221;, &#8220;honesty&#8221; and &#8220;competence&#8221; &#8212;
and how those traits can be used to predict the outcomes of political elections, job hires, and
marriage engagements. The authors propose a hierarchical model for enduring traits inferred from
faces, incorporating high-level perceptions and intermediate-level attributes. Surprisingly, or not so
surprisingly, they show that the trained model can successfully classify the outcomes of two
                                                                                         
                                                                                         
important political events, only using the photographs of politicians&#8217; faces. Firstly, it classifies the
winners of a series of U.S. elections with the accuracy of 67<span 
class="cmmi-12">.</span>9% (Governors) and 65<span 
class="cmmi-12">.</span>5% (Senators).
By interpreting the pipeline, they find that different political offices require different types of
preferred traits. Secondly, the model can categorize the political party affiliations of politicians, i.e.,
Democrats vs. Republicans, with the accuracy of 62<span 
class="cmmi-12">.</span>6% (male) and 60<span 
class="cmmi-12">.</span>1% (female).
This study appears to be the first to use automated visual trait analysis to predict the
outcomes of real-world social events. Further, the proposed approach is more scalable
and objective than the prior behavioral studies, and opens for a range of new exciting
applications.
<!--l. 673--><p class="indent" >   <a 
href="#Xhuang2016inferring">Huang and Kovashka</a>&#x00A0;(<span class="bib-year"><a 
href="#Xhuang2016inferring">2016</a></span>) extend <a 
href="#Xjoo2015automated">Joo et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xjoo2015automated">2015</a></span>) by exploring a variety of features for
predicting communicative intents. They study a number of facial expressions and body poses as
cues for the implied nuances of the politician&#8217;s personality, as well as how the environmental
settings such as kitchen or hospital influence the audience&#8217;s perception of the portrayed politician.
They improve the performance by learning intermediate cues using convolutional neural
networks and document state-of-the-art results on the <span 
class="cmti-12">Visual Persuasion </span>dataset of <a 
href="#Xjoo2015automated">Joo et
al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xjoo2015automated">2015</a></span>).
<!--l. 683--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">3.4   </span> <a 
 id="x1-120003.4"></a>Infographics</h4>
<!--l. 685--><p class="noindent" ><a 
href="#Xbylinskii2017learning">Bylinskii, Kim, Donovan, Alsheikh, Madan, Pfister, Durand, Russell and Hertzmann</a>&#x00A0;(<span class="bib-year"><a 
href="#Xbylinskii2017learning">2017a</a></span>)
introduce automated models that predict the relative importance of different elements in data
visualizations and graphic designs. They use neural networks trained on human clicks and
importance annotations on hundreds of designs. The authors collected a dataset of crowdsourced
importance, and analyzed the predictions of their neural network models with respect to ground
truth importance and human eye movements. Therefore they demonstrate how such predictions of
importance can be used for automatic design retargeting and thumbnailing. The validity of their
method was established by user studies with hundreds of MTurk participants. With limited
post-processing, the proposed importance-driven applications are on par with, or outperform,
state-of-the-art methods, including natural image saliency. The importance predictions can also
be built into interactive design tools to offer immediate feedback during the design
process.
<!--l. 700--><p class="indent" >   <a 
href="#Xbylinskii2017understanding">Bylinskii, Alsheikh, Madan, Recasens, Zhong, Pfister, Durand and Oliva</a>&#x00A0;(<span class="bib-year"><a 
href="#Xbylinskii2017understanding">2017b</a></span>) introduce the
                                                                                         
                                                                                         
problem of visual hashtag discovery for infographics: extracting visual elements from an
infographic that are diagnostic of its topic. Based on a curated dataset of 29K large
infographic images sampled across 26 categories and 391 tags, the proposed automated
method consists of two steps: first, extract the text from an infographic and use it to
predict text tags indicative of the infographic content; second, use these predicted text
tags as a supervisory signal to localize the most diagnostic visual elements from within
the infographic, which they call &#8220;visual hashtags&#8221;. They exemplify how textual and
visual elements can be used to jointly reason about the high-level topics (categories) of
infographics, as well as the finer-grained sub-topics (tags). In addition, they demonstrate the
power of text features in disambiguating and providing context for visual features. Their
performances on a categorization and multi-label tag prediction were compared to human
annotations.
<!--l. 716--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">3.5   </span> <a 
 id="x1-130003.5"></a>Style Detection</h4>
<!--l. 720--><p class="noindent" >There is a fast growing body of research that aims at learning a notion of styles from
images, whether it be art (<a 
href="#Xkarayev2013recognizing">Karayev, Trentacoste, Han, Agarwala, Darrell, Hertzmann and
Winnemoeller</a>&#x00A0;<a 
href="#Xkarayev2013recognizing">2013</a>,&#x00A0;<a 
href="#Xliu2015inferring">Liu, Yan, Ricci, Yang, Han, Winkler and Sebe</a>&#x00A0;<a 
href="#Xliu2015inferring">2015</a>) (including the explosion
of studies on style transfer, following the seminal work of <a 
href="#Xgatys2015neural">Gatys, Ecker and Bethge</a>&#x00A0;(<span class="bib-year"><a 
href="#Xgatys2015neural">2015</a></span>) or
<a 
href="#Xgatys2016image">Gatys, Ecker and Bethge</a>&#x00A0;(<span class="bib-year"><a 
href="#Xgatys2016image">2016</a></span>), which we review in Section&#x00A0;<a 
href="#x1-160003.6.2">3.6.2<!--tex4ht:ref: subsubsec-style-transfer --></a>), vehicle (<a 
href="#XLee_2013_ICCV">Jae&#x00A0;Lee, Efros and
Hebert</a>&#x00A0;<a 
href="#XLee_2013_ICCV">2013</a>), scenic spots (<a 
href="#Xdoersch2012what">Doersch, Singh, Gupta, Sivic and Efros</a>&#x00A0;<a 
href="#Xdoersch2012what">2012</a>,&#x00A0;<a 
href="#XQuercia2014AestheticCW">Quercia, O&#8217;Hare
and Cramer</a>&#x00A0;<a 
href="#XQuercia2014AestheticCW">2014</a>), photograph (<a 
href="#Xthomas2016seeing">Thomas and Kovashka</a>&#x00A0;<a 
href="#Xthomas2016seeing">2016</a>), and clothing (<a 
href="#Xbossard2012apparel">Bossard,
Dantone, Leistner, Wengert, Quack and Van&#x00A0;Gool</a>&#x00A0;<a 
href="#Xbossard2012apparel">2012</a>,&#x00A0;<a 
href="#Xveit2015learning">Veit, Kovacs, Bell, McAuley,
Bala and Belongie</a>&#x00A0;<a 
href="#Xveit2015learning">2015</a>,&#x00A0;<a 
href="#Xkiapour2014hipster">Kiapour, Yamaguchi, Berg and Berg</a>&#x00A0;<a 
href="#Xkiapour2014hipster">2014</a>), which we review
below.
<!--l. 729--><p class="indent" >   <span 
class="cmti-12">Learning styles of art. </span><a 
href="#XGarces2014ASM">Garces, Agarwala, Gutierrez and Hertzmann</a>&#x00A0;(<span class="bib-year"><a 
href="#XGarces2014ASM">2014</a></span>) present a method
for measuring the similarity in style between two pieces of vector art, independent of content.
Similarity is measured by the differences between four types of features: color, shading, texture,
and stroke. Feature weightings are learned from crowdsourced experiments. This perceptual
similarity enables style-based search, with which they demonstrate an application that allows
users to create stylistically-coherent clip art mash-ups. (<a 
href="#Xkarayev2013recognizing">Karayev et al.</a>&#x00A0;<a 
href="#Xkarayev2013recognizing">2013</a>,&#x00A0;<a 
href="#Xliu2015inferring">Liu et
al.</a>&#x00A0;<a 
href="#Xliu2015inferring">2015</a>)
                                                                                         
                                                                                         
<!--l. 738--><p class="indent" >   <span 
class="cmti-12">Learning city icons. </span><a 
href="#Xdoersch2012what">Doersch et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xdoersch2012what">2012</a></span>) seek to automatically find visual elements that are
most distinctive for a certain geo-spatial area, for example the city of Paris, given a large
repository of geotagged imagery. A discriminative clustering approach is proposed to show that
geographically representative image elements can be discovered automatically from Google Street
View imagery. They demonstrate that these elements are visually interpretable and
perceptually geo-informative, for the purpose of tourism marketing of Paris. The discovered
visual elements can also support a variety of computational geography tasks, such as
mapping architectural correspondences and influences within and across cities, finding
representative elements at different geo-spatial scales, and geographically-informed image
retrieval.
<!--l. 752--><p class="indent" >   <a 
href="#XQuercia2014AestheticCW">Quercia et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#XQuercia2014AestheticCW">2014</a></span>) present a crowdsourcing project that aims to investigate, at scale, which
visual aspects of a city neighborhoods (e.g. London) make them appear beautiful, quiet, and/or
happy. They collected votes from over 3.3K individuals and translate them into quantitative
measures of urban perception, thereby quantifying each neighborhood&#8217;s aesthetic capital. By then
using state-of-the-art image processing techniques, the authors were able to determine visual cues
that may cause a street to be perceived as being beautiful, quiet, or happy. Effects of color, texture
and visual words were identified. For example, the amount of greenery is the most positively
associated visual cue with each of three qualities; by contrast, broad streets, fortress-like
buildings, and council houses tend to be associated with the opposite qualities (ugly,
noisy, and unhappy). Such insights are especially useful for marketing purposes in the
travel industry, as well as the visualization city identities and the personification of
cities.
<!--l. 769--><p class="indent" >   <span 
class="cmti-12">Learning styles of photos. </span>(<a 
href="#Xthomas2016seeing">Thomas and Kovashka</a>&#x00A0;<a 
href="#Xthomas2016seeing">2016</a>) introduce the novel problem of
identifying the photographer behind a photograph. They created a dataset of over 180,000 images
taken by 41 well-known photographers, and examined the effectiveness of a variety of features (low
and high-level, including CNN features) at identifying the photographer. They also trained a deep
convolutional neural network tailored for this task. They show that high-level features greatly
outperform low-level features.
<!--l. 778--><p class="indent" >   <span 
class="cmti-12">Learning vehicle style. </span><span 
class="cmbx-12">? </span>present a weakly-supervised visual data mining approach that
discovers connections between recurring midlevel visual elements in historic (temporal) and
geographic (spatial) image collections, and attempts to capture the underlying visual
style. In contrast to existing discovery methods that mine for patterns that remain
visually consistent throughout the dataset, they discover visual elements whose appearance
                                                                                         
                                                                                         
changes due to change in time or location; i.e., exhibit consistent stylistic variations across
the label space (date or geo-location). Their approach first identify groups of patches
that are style sensitive; it then incrementally builds correspondences to find the same
element across the entire dataset. Finally, they train style-aware regressors that model
each element&#8217;s range of stylistic differences. They apply it to date and geo-location
prediction and show substantial improvement over several baselines that do not model visual
style. The method&#8217;s effectiveness is also demonstrated on the related task of fine-grained
classification.
<!--l. 796--><p class="indent" >   <span 
class="cmti-12">Learning clothing style</span>. <a 
href="#Xmurillo2012urban">Murillo, Kwak, Bourdev, Kriegman and Belongie</a>&#x00A0;(<span class="bib-year"><a 
href="#Xmurillo2012urban">2012</a></span>) consider
photos of groups of people to learn which groups are more likely to socialize with one another. This
implies learning a distance metric between images. However, they require manually specified styles,
called &#8220;urban tribes&#8221;. Similarly, <a 
href="#Xbossard2012apparel">Bossard et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xbossard2012apparel">2012</a></span>) who use a random forest approach to classify
the style of clothing images, require pre-specified classes of style. <a 
href="#Xvittayakorn2015runway">Vittayakorn, Yamaguchi, Berg
and Berg</a>&#x00A0;(<span class="bib-year"><a 
href="#Xvittayakorn2015runway">2015</a></span>) learn outfit similarity, based on specific descriptors for color, texture and shape.
Therefore their system is able to retrieve similar outfits to a query image. <a 
href="#Xmcauley2015image">McAuley,
Targett, Shi and Van Den&#x00A0;Hengel</a>&#x00A0;(<span class="bib-year"><a 
href="#Xmcauley2015image">2015</a></span>) collect a large scale co-purchase dataset from
<a 
href="https://www.amazon.com" >Amazon</a>and learn a notion of style and retrieve products from different categories that are
supposed to be of similar style, using the image features from AlexNet (<a 
href="#XNIPS2012_4824">Krizhevsky et
al.</a>&#x00A0;<a 
href="#XNIPS2012_4824">2012</a>) that was trained for object classification to learn their distance metric. <a 
href="#Xveit2015learning">Veit et
al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xveit2015learning">2015</a></span>) address the exact same problem. Rather than using logistic regression, <a 
href="#Xveit2015learning">Veit et
al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xveit2015learning">2015</a></span>) advance the method by fine-tuning the entire network with a Siamese architecture
with a different sampling strategy, which they claim to be less prone to the cold-start
problem in the previous paper. More specifically, <a 
href="#Xveit2015learning">Veit et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xveit2015learning">2015</a></span>) propose a framework to
learn a feature transformation from images of items into a latent space that expresses
compatibility. For the feature transformation, a Siamese Convolutional Neural Network (CNN)
architecture is used, where training examples are pairs of items that are either compatible or
incompatible. The authors model compatibility based on large-scale user co-purchase
data from <a 
href="https://www.amazon.com" >Amazon</a>, as do <a 
href="#Xmcauley2015image">McAuley et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xmcauley2015image">2015</a></span>). <a 
href="#Xveit2015learning">Veit et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xveit2015learning">2015</a></span>) construct pairs of
heterogeneous dyads as training data in order to learn cross-category fit. They demonstrate that
the proposed framework is capable of learning semantic information about visual style
and is able to generate outfits of clothes, with items from different categories, that are
esthetically compatible. <a 
href="#Xkiapour2014hipster">Kiapour et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xkiapour2014hipster">2014</a></span>) further extend the identified style of
clothing to personal wealth, occupation, and socio-identity, based on their assumption or
                                                                                         
                                                                                         
observation that the clothing we wear and our identities are closed tied, revealing to the
world clues other aspects of our lives. They designed an online competitive Style Rating
Game called <span 
class="cmti-12">Hipster Wars </span>to crowd source reliable human judgments of style, with
which they collected a dataset of clothing outfits with associated style ratings for 5
style categories: hipster, bohemian, pinup, preppy, and goth. They train models for
between-class and within-class classification of styles, and thus identifying clothing elements
that are generally discriminative for a style, as well as items in a particular outfit that
may indicate a style. <a 
href="#Xyamaguchi2013paper">Yamaguchi, Hadi&#x00A0;Kiapour and Berg</a>&#x00A0;(<span class="bib-year"><a 
href="#Xyamaguchi2013paper">2013</a></span>) introduce an effective
retrieval-based clothing parising method along with a large annotated dataset of fashion
photos. For a query image, they find similar styles from a large database of tagged fashion
images and use these examples to parse the query. Their approach combines parsing
from: pre-trained global clothing models, local clothing models learned on the fly from
retrieved examples, and transferred parse masks (paper doll item transfer) from retrieved
examples. They show that this approach significantly outperforms state-of-the-art in parsing
accuracy.
<!--l. 850--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">3.6   </span> <a 
 id="x1-140003.6"></a>Fashion, Art, and Design</h4>
<!--l. 852--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">3.6.1   </span> <a 
 id="x1-150003.6.1"></a>Fashion</h5>
<!--l. 854--><p class="noindent" >Computer vision systems are designed to work well within the context of everyday photography.
However, artists often render the world around them in ways that do not resemble photographs.
Artwork produced by people is not constrained to mimic the physical world, making it more
challenging for machines to recognize.
<!--l. 860--><p class="indent" >   <a 
href="#Xchen2006composite">Chen, Xu, Liu and Zhu</a>&#x00A0;(<span class="bib-year"><a 
href="#Xchen2006composite">2006</a></span>) used an And-Or representation to build a tree of
composite clothing templates by gathering and segmenting artists&#8217; sketches and match
those clothing template to the image. This is one of the first studies on clothing in the
community.
<!--l. 865--><p class="indent" >   <a 
href="#Xveit2015learning">Veit et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xveit2015learning">2015</a></span>) propose a learning framework to recommend matching articles of clothing to
consumers. The type of questions their system is trained to answer are along the lines of &#8220;What
                                                                                         
                                                                                         
outfit goes well with this pair of shoes?&#8221; The idea of this framework is to learn a feature
transformation from images of items into a latent space that expresses compatibility. For the
feature transformation, a Siamese Convolutional Neural Network (CNN) architecture is used,
where training examples are pairs of items that are either compatible or incompatible. The authors
model compatibility based on large-scale user co-purchase data from <a 
href="https://www.amazon.com" >Amazon</a>. They construct pairs
of heterogeneous dyads as training data in order to learn cross-category fit. They demonstrate that
the proposed framework is capable of learning semantic information about visual style and is able
to generate outfits of clothes, with items from different categories, that are esthetically
compatible.
<!--l. 881--><p class="indent" >   <a 
href="#Xvittayakorn2015runway">Vittayakorn et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xvittayakorn2015runway">2015</a></span>) documents the first attempt to provide a quantitative analysis of
fashion on the runway and in the streets. They release a large-scale dataset of runway fashion
photos representing 9<span 
class="cmmi-12">, </span>328 fashion shows over 15 years, with which they develop a feature
representation that can usefully capture the appearance of clothing items in outfits,
through pose estimation, clothing parsing, and feature extraction. They collected human
judgments of outfit similarity to train models for predicting similarity between runway
outfits and street outfits. They found their proposed representation boosts prediction
performance of the season, year, and brand, outperforming humans. [Nearest Neighbor, feature
representation]
<!--l. 893--><p class="indent" >   <a 
href="#Xbossard2012apparel">Bossard et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xbossard2012apparel">2012</a></span>) build a pipeline for recognizing and classifying clothing in a natural
setting, combining upper body detectors, style classification, attribute classification in a Random
Forest. <a 
href="#Xliu2012street">Liu, Song, Liu, Xu, Lu and Yan</a>&#x00A0;(<span class="bib-year"><a 
href="#Xliu2012street">2012b</a></span>) develop a system that takes a daily street
snapshot of individuals, and efficiently searches online for articles of clothing similar to the outfit in
the street photo. It is a problem of cross-scenario clothing retrieval &#8212; the core of which
lies in correctly identifying similarities between clothings against large discrepancies
between street photos and online catalogue photos due to distinct human posture and
environmental background. The proposed solution leverages human pose estimation
and offline structure detection. <a 
href="#Xchao2009framework">Chao et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xchao2009framework">2009</a></span>) present a clothing recommendation
system called the Smart Mirror. They use computer vision to recognize classes and
attributes of clothing for personal fashion recommendation, mimicking a real-time customer
fashion assistant in a store&#8217;s fitting room. Figure&#x00A0;<a 
href="#x1-15001r2">2<!--tex4ht:ref: fig:chao2009 --></a> showcases a prototype of the smart
mirror. The end user stands in front of a mirror-TV with an embedded web camera.
First a users&#8217;s image is captured and the face region is detected automatically, based
on which a rectangular region of interest (ROI) is selected to characterize the current
                                                                                         
                                                                                         
clothing style. Using various image descriptors for ROI, similar clothes in style can be
found in a database to be displayed to the user by the mirror-TV for inspiration. A
slightly more marketing version of a garment recommendation system was published in
Marketing Science <span 
class="cmti-12">seven </span>years later (<a 
href="#Xlu2016video">Lu et al.</a>&#x00A0;<a 
href="#Xlu2016video">2016</a>), which we detailed in Section&#x00A0;<a 
href="#x1-20002">2<!--tex4ht:ref: sec-cv-mkg --></a>.
<hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         <a 
 id="x1-15001r2"></a>
                                                                                         
                                                                                         
<!--l. 921--><p class="noindent" ><img 
src="1_Users_sheng_Dropbox_AAA_survey_images_chao2009.png" alt="PIC"  
>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;2: </span><span  
class="content">Prototype of the Smart Mirror</span></div><!--tex4ht:label?: x1-15001r2 -->
                                                                                         
                                                                                         <!--l. 924--><p class="indent" ></div><hr class="endfigure">
   <h5 class="subsubsectionHead"><span class="titlemark">3.6.2   </span> <a 
 id="x1-160003.6.2"></a>Style Transfer</h5>
<!--l. 929--><p class="noindent" >The body of literature on style transfer grew rapidly since the seminal paper of <a 
href="#Xgatys2015neural">Gatys et
al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xgatys2015neural">2015</a></span>), i.e., <a 
href="#Xgatys2016image">Gatys et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xgatys2016image">2016</a></span>), first emerged in 2015.
<!--l. 933--><p class="indent" >   <a 
href="#Xgatys2015neural">Gatys et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xgatys2015neural">2015</a></span>) (or <a 
href="#Xgatys2016image">Gatys et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xgatys2016image">2016</a></span>)) introduce an artificial system based on a Deep
Neural Network that creates artistic images of high perceptual quality. The system uses neural
representations to separate and recombine <span 
class="cmti-12">content </span>and <span 
class="cmti-12">style </span>of arbitrary images, providing a neural
algorithm for the creation of artistic images. Moreover, in light of the striking similarities between
performance-optimised artificial neural networks and biological vision, the study offers a path
forward to an algorithmic understanding of how humans create and perceive artistic
imagery.
<!--l. 943--><p class="indent" >   In the aforementioned work, feed-forward convolutional neural networks are trained using a
per-pixel loss between the output and ground-truth images. Meanwhile, parallel work has shown
that high-quality images can be generated by defining and optimizing perceptual loss functions
based on high-level features extracted from pretrained networks. <a 
href="#Xjohnson2016perceptual">Johnson, Alahi and Fei-Fei</a>&#x00A0;(<span class="bib-year"><a 
href="#Xjohnson2016perceptual">2016</a></span>)
propose to combine the benefits of both approaches, and demonstrate the use of perceptual loss
functions for training feed-forward networks for image transformation tasks. They showcase
how to solve the optimization problem underlying <a 
href="#Xgatys2015neural">Gatys et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xgatys2015neural">2015</a></span>) in real-time.
Compared to the optimization-based method, the network in <a 
href="#Xjohnson2016perceptual">Johnson et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xjohnson2016perceptual">2016</a></span>) gives
similar qualitative results but is three orders of magnitude faster. Extra experiments with
single-image super-resolution, where replacing a per-pixel loss with a perceptual loss also give
rise to visually pleasing results. Local style transfer algorithms are based on spatial
color mappings, more expressive, and can handle a broad class of applications such as
painterly transfer (<a 
href="#Xselim2016painting">Selim, Elgharib and Doyle</a>&#x00A0;<a 
href="#Xselim2016painting">2016</a>,&#x00A0;<a 
href="#Xli2016combining">Li and Wand</a>&#x00A0;<a 
href="#Xli2016combining">2016</a>). The approach,
as is, is not suitable for photo realistic style transfer. Even when both the input and
reference images are photographs, the output still exhibits distortions reminiscent of a
painting. <a 
href="#Xluan2017deep">Luan, Paris, Shechtman and Bala</a>&#x00A0;(<span class="bib-year"><a 
href="#Xluan2017deep">2017</a></span>) introduce a deep-learning approach to
photographic style transfer that handles a large variety of image content while faithfully
transferring the reference style. Their contribution is to constrain the transformation
from the input to the output to be locally affine in color space, and to express this
constraint as a custom fully differentiable energy term. They show that this approach
successfully suppresses distortion and yields satisfying photo realistic style transfers in a broad
                                                                                         
                                                                                         
variety of scenarios, including transfer of the time of day, weather, season, and artistic
edits.
<!--l. 972--><p class="indent" >   We believe that the methods proposed in this body of literature are especially suitable for, and
could be readily applied to marketing applications, such as automatically balancing content
and style in designs with respect to brand image, company logo, product packaging,
etc.
<!--l. 978--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">4.   </span> <a 
 id="x1-170004"></a>A Summary of Public or Author-released Datasets</h3>
<!--l. 981--><p class="noindent" >
      <ol  class="enumerate1" >
      <li 
  class="enumerate" id="x1-17002x1"><a 
href="#Xwilber2017bam">Wilber, Fang, Jin, Hertzmann, Collomosse and Belongie</a>&#x00A0;(<span class="bib-year"><a 
href="#Xwilber2017bam">2017</a></span>) collected a large-scale
      dataset of contemporary artwork from Behance, a website containing millions of portfolios
      from professional and commercial artists. The resulting dataset which they termed &#8220;<span 
class="cmbx-12">the</span>
      <span 
class="cmbx-12">Behance Artistic Media Dataset</span>&#8221;, containing almost 65 million images and quality
      assurrance thresholds, is available at <a 
href="https://bam-dataset.org/" class="url" ><span 
class="cmtt-12">https://bam-dataset.org/</span></a>. They also create an
      expert defined vocabulary of binary artistic attributes that spans the broad spectrum of
      artistic styles and content represented in the dataset:
           <ul class="itemize1">
           <li class="itemize">Media attributes: they label images created in 3D computer graphics, comics, oil
           painting, pen ink, pencil sketches, vector art, and watercolor;
           </li>
           <li class="itemize">Emotion attributes: they label images that are likely to make the viewer feel
           calm/peaceful, happy/cheerful, sad/gloomy, and scary/fearful;
           </li>
           <li class="itemize">Entry-level  object  category  attributes:  they  label  images  containing  bicycles,
           birds, buildings, cars, cats, dogs, flowers, people, and trees.</li></ul>
      <!--l. 999--><p class="noindent" >Furthermore, we carry out baseline experiments to show the value of this dataset for artistic
      style prediction, for improving the generality of existing object classifiers, and for the study
      of visual domain adaptation.
                                                                                         
                                                                                         </li>
      <li 
  class="enumerate" id="x1-17004x2">From analyzing architectural prints, pictures of fashion designs in magazines, to going into
      the field to take pictures of products currently in the market, researchers have gone the
      length to get data to build an understanding of product design. One important archival
      source containing hundreds of thousands of designs is the US design patent database. The US
      design patent chronicles the creation of new product forms patented in the US since 1842.
      The United States Patent and Trademark Office (USPTO) makes data on patents publicly
      accessible. <a 
href="#Xchan2017styles">Chan et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xchan2017styles">2017</a></span>) maintain this large-scale public US design patent dataset at
      <a 
href="http://www.stylesinproductdesign.com/data" class="url" ><span 
class="cmtt-12">http://www.stylesinproductdesign.com/data</span></a>.
      </li>
      <li 
  class="enumerate" id="x1-17006x3"><a 
href="#Xmei2012imagesense">Mei et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xmei2012imagesense">2012</a></span>) released a unique advertisement-image dataset that consists of 7<span 
class="cmmi-12">, </span>285
      unique ad product logos with annotations of 32<span 
class="cmmi-12">, </span>480 unique ad words done by 20 subjects, as
      well as 382<span 
class="cmmi-12">, </span>371 images from <a 
href="http://www.tango.msra" class="url" ><span 
class="cmtt-12">http://www.tango.msra</span></a> and 200<span 
class="cmmi-12">, </span>000 images from <a 
href="http://www.flickr.com" >Flickr</a>, based
      on which they evaluate their proposed ad targeting system ImageSense with 1<span 
class="cmmi-12">, </span>100 ad
      triggering pages (100 web pages from major news sites, 1<span 
class="cmmi-12">, </span>000 images searched by top 100
      image queries).
      </li>
      <li 
  class="enumerate" id="x1-17008x4"><a 
href="#Xhussain2017automatic">Hussain et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xhussain2017automatic">2017</a></span>) released two valuable datasets of advertisements:
           <ol  class="enumerate2" >
           <li 
  class="enumerate" id="x1-17010x1">an image dataset of 64,832 image ads;
           </li>
           <li 
  class="enumerate" id="x1-17012x2">a video dataset of 3,477 ads.</li></ol>
      <!--l. 1027--><p class="noindent" >Both contain rich annotations of the topics, the sentiments, questions and answers describing
      the objectives, the reasoning the ad presents to persuade the viewer, as well as the symbolic
      references ads make.
      </li>
      <li 
  class="enumerate" id="x1-17014x5"><a 
href="#Xrohrbach2015dataset">Rohrbach et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xrohrbach2015dataset">2015</a></span>) released a dataset of transcribed Descriptive video service (DVS),
      temporally aligned to full length HD movies, and supplemented with the aligned movie
      scripts. DVS provides linguistic descriptions of movies and are by design mainly visual. In
      total the Movie Description dataset contains a parallel corpus of over 54,000 sentences and
      video snippets from 72 HD movies, building on which, <a 
href="#Xtapaswi2016movieqa">Tapaswi et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xtapaswi2016movieqa">2016</a></span>) introduce the
                                                                                         
                                                                                         
      MovieQA dataset consisting of 14,944 questions about 408 movies with high semantic
      diversity. Each question comes with a set of five possible answers; a correct one and four
      deceiving answers provided by human annotators. Multiple sources of information â
      video clips, plots, subtitles, scripts, and DVS (<a 
href="#Xrohrbach2015dataset">Rohrbach et al.</a>&#x00A0;<a 
href="#Xrohrbach2015dataset">2015</a>) were all
      included.
      </li>
      <li 
  class="enumerate" id="x1-17016x6"><a 
href="#Xbylinskii2017learning">Bylinskii et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xbylinskii2017learning">2017a</a></span>) introduced a curated dataset of 29K large infographic images
      sampled across 26 categories and 391 tags for training deep learning models for visual
      summarization, which they call &#8220;visual hashtags&#8221;.
      </li>
      <li 
  class="enumerate" id="x1-17018x7"><a 
href="#Xthomas2016seeing">Thomas and Kovashka</a>&#x00A0;(<span class="bib-year"><a 
href="#Xthomas2016seeing">2016</a></span>) created a dataset of over 180,000 images taken by 41
      well-known photographers, exhibiting various artistic photographic styles.
      </li>
      <li 
  class="enumerate" id="x1-17020x8"><a 
href="#Xmcauley2015image">McAuley et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xmcauley2015image">2015</a></span>) introduced a large scale co-purchase dataset gathered from <a 
href="https://www.amazon.com" >Amazon</a>for
      the learning task of a notion of style. The dataset contains over 180 million relationships
      between a pool of almost 6 million objects. These relationships are a result of visiting
      Amazon and recording the product recommendations that it provides given our
      (apparent) interest in the subject of a particular web page. The statistics of the
      dataset shown in Table 1 of the paper is reproduced in Table&#x00A0;<a 
href="#x1-17021r1">1<!--tex4ht:ref: tab:tab1 --></a>. An image and a
      category label are available for each object, as is the set of users who reviewed
      it.
      <div class="table">
                                                                                         
                                                                                         
      <!--l. 1060--><p class="noindent" ><a 
 id="x1-17021r1"></a><hr class="float"><div class="float" 
> <!--tex4ht:inline--><div class="tabular">
 <table id="TBL-1" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-1-1g"><col 
id="TBL-1-1"><col 
id="TBL-1-2"><col 
id="TBL-1-3"><col 
id="TBL-1-4"><col 
id="TBL-1-5"></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-1-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-1-1"  
class="td01"> Category                       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-1-2"  
class="td11"> Users        </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-1-3"  
class="td11"> Items      </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-1-4"  
class="td11"> Ratings       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-1-5"  
class="td10"> Edges       </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-2-1"  
class="td01"> Books                            </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-2-2"  
class="td11"> 8,201,127   </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-2-3"  
class="td11"> 1,606,219  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-2-4"  
class="td11"> 25,875,237   </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-2-5"  
class="td10"> 51,276,522  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-3-1"  
class="td01"> Cell Phones &amp; Accessories  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-3-2"  
class="td11"> 2,296,534   </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-3-3"  
class="td11"> 223,680    </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-3-4"  
class="td11"> 5,929,668     </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-3-5"  
class="td10"> 4,485,570   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-4-1"  
class="td01"> Clothing, Shoes &amp; Jewelry  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-4-2"  
class="td11"> 3,260,278   </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-4-3"  
class="td11"> 773,465    </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-4-4"  
class="td11"> 25,361,968   </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-4-5"  
class="td10"> 16,508,162  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-5-1"  
class="td01"> Digital Music                   </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-5-2"  
class="td11"> 490,058     </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-5-3"  
class="td11"> 91,236     </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-5-4"  
class="td11"> 950,621       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-5-5"  
class="td10"> 1,615,473   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-6-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-6-1"  
class="td01"> Electronics                      </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-6-2"  
class="td11"> 4,248,431   </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-6-3"  
class="td11"> 305,029    </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-6-4"  
class="td11"> 11,355,142   </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-6-5"  
class="td10"> 7,500,100   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-7-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-7-1"  
class="td01"> Grocery &amp; Gourmet Food  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-7-2"  
class="td11"> 774,095     </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-7-3"  
class="td11"> 120,774    </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-7-4"  
class="td11"> 1,997,599     </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-7-5"  
class="td10"> 4,452,989   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-8-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-8-1"  
class="td01"> Home &amp; Kitchen              </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-8-2"  
class="td11"> 2,541,693   </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-8-3"  
class="td11"> 282,779    </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-8-4"  
class="td11"> 6,543,736     </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-8-5"  
class="td10"> 9,240,125   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-9-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-9-1"  
class="td01"> Movies &amp; TV                  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-9-2"  
class="td11"> 2,114,748   </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-9-3"  
class="td11"> 150,334    </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-9-4"  
class="td11"> 6,174,098     </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-9-5"  
class="td10"> 5,474,976   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-10-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-10-1"  
class="td01"> Musical Instruments          </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-10-2"  
class="td11"> 353,983     </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-10-3"  
class="td11"> 65,588     </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-10-4"  
class="td11"> 596,095       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-10-5"  
class="td10"> 1,719,204   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-11-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-11-1"  
class="td01"> Office Products                </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-11-2"  
class="td11"> 919,512     </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-11-3"  
class="td11"> 94,820     </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-11-4"  
class="td11"> 1,514,235     </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-11-5"  
class="td10"> 3,257,651   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-12-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-12-1"  
class="td01"> Toys &amp; Games                 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-12-2"  
class="td11"> 1,352,110   </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-12-3"  
class="td11"> 259,290    </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-12-4"  
class="td11"> 2,386,102     </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-12-5"  
class="td10"> 13,921,925  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-13-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-13-1"  
class="td01"> Total                             </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-13-2"  
class="td11"> 20,980,320  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-13-3"  
class="td11"> 5,933,184  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-13-4"  
class="td11"> 143,663,229  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-1-13-5"  
class="td10"> 180,827,502</td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-14-"><td  style="white-space:nowrap; text-align:left;" id="TBL-1-14-1"  
class="td01">                        </td></tr></table></div>
<br /> <div class="caption" 
><span class="id">Table&#x00A0;1: </span><span  
class="content">The types of objects from a few categories in the dataset and the number of
relationships between them.</span></div><!--tex4ht:label?: x1-17021r1 -->
      </div><hr class="endfloat" />
      </div>
      </li>
      <li 
  class="enumerate" id="x1-17023x9"><a 
href="#Xyamaguchi2012parsing">Yamaguchi, Kiapour, Ortiz and Berg</a>&#x00A0;(<span class="bib-year"><a 
href="#Xyamaguchi2012parsing">2012</a></span>) introduced <span 
class="cmti-12">the Fashionista dataset</span>, consisting of
      158<span 
class="cmmi-12">, </span>235 fashion photos with associated text annotations, and web-based tools for labeling.
      The dataset was collected from <a 
href="Chictopia.com" class="url" ><span 
class="cmtt-12">Chictopia.com</span></a>, a social networking website for fashion
      bloggers. On this website, fashionistas upload &#8220;outfit of the day&#8221; type pictures, designed to
      draw attention to their fashion choices or as a form of social interaction with
      peers. They tend to display a wide range of styles, accessories, and garments. In
      addition, the pictures are also often depicted in relatively simple poses (mostly
      standing), against relatively clean backgrounds, and without many other people in the
      picture.
      </li>
      <li 
  class="enumerate" id="x1-17025x10"><a 
href="#Xyamaguchi2013paper">Yamaguchi et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xyamaguchi2013paper">2013</a></span>) introduced <span 
class="cmti-12">The Paper Doll dataset</span>, which is a large, complex, real
      world collection of tagged outfit pictures from the aforementioned social network focused on
      fashion, <a 
href="chictopia.com" class="url" ><span 
class="cmtt-12">chictopia.com</span></a>. It consists of over 1 million pictures from <a 
href="https://www.chictopia.com" >Chictopia</a>with associated
      metadata tags denoting characteristics such as color, clothing item, or occasion. Since
      <span 
class="cmti-12">the Fashionista dataset </span>also uses Chictopia, <span 
class="cmti-12">the Paper Doll dataset </span>exclude any
                                                                                         
                                                                                         
      duplicate pictures from <span 
class="cmti-12">the Fashionista dataset</span>. From the remaining, They selected
      pictures tagged with at least one clothing item and ran a full-body pose detector
      (<a 
href="#Xyang2011articulated">Yang and Ramanan</a>&#x00A0;<a 
href="#Xyang2011articulated">2011</a>), keeping those that have a person detection. This
      resulted in 339<span 
class="cmmi-12">, </span>797 pictures weakly annotated with clothing items and estimated
      pose.
      </li>
      <li 
  class="enumerate" id="x1-17027x11"><a 
href="#Xkiapour2014hipster">Kiapour et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xkiapour2014hipster">2014</a></span>) crowd-sourced reliable human judgments of clothing styles, and
      therefore introduced a dataset of clothing outfits with associated style ratings for 5 style
      categories: hipster, bohemian, pinup, preppy, and goth.
      </li>
      <li 
  class="enumerate" id="x1-17029x12"><a 
href="#Xvittayakorn2015runway">Vittayakorn et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xvittayakorn2015runway">2015</a></span>) released a large-scale dataset containing 348,598 runway fashion
      photos representing 9<span 
class="cmmi-12">, </span>328 fashion shows over 15 years, combined with the Paper Doll dataset
      released by <a 
href="#Xyamaguchi2013paper">Yamaguchi et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xyamaguchi2013paper">2013</a></span>).</li></ol>
<!--l. 1181--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">5.   </span> <a 
 id="x1-180005"></a>Overlap, Gap, and Opportunities</h3>
<!--l. 1183--><p class="noindent" >There appear to be some significant interest overlap revolving around research topics such as
advertising, motion pictures, fashion, emotions, etc.
<!--l. 1187--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">5.1   </span> <a 
 id="x1-190005.1"></a>Advertising</h4>
<!--l. 1189--><p class="noindent" >Advertising studies in computer vision appear to be more focused on automatic understanding and
reasoning about existing advertisements at the moment, whereas advertising in marketing is more
concerned with the effect of various aspects of advertising on some market outcomes, whether it be
sales, consumer engagement, revenue, market share, etc., with exceptions in both cases. We
argue that automatic ad understanding, for instance, in the form of Visual Question
Answering (VQA) (<a 
href="#Xhussain2017automatic">Hussain et al.</a>&#x00A0;<a 
href="#Xhussain2017automatic">2017</a>), could be integrated with discrete choice models and
datasets associated with consumer response and market outcomes, to create end-to-end
advertisement generation systems that tailor advertisements shown to consumer preferences and
design personalized advertisements for each consumer, in order to maximize marketing
                                                                                         
                                                                                         
return.
<!--l. 1202--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">5.2   </span> <a 
 id="x1-200005.2"></a>Movies</h4>
<!--l. 1204--><p class="noindent" >Similarly, movie studies in computer vision are more concerned about reasoning and understanding
movie plots based on scripts and frames (<a 
href="#Xrohrbach2015dataset">Rohrbach et al.</a>&#x00A0;<a 
href="#Xrohrbach2015dataset">2015</a>,&#x00A0;<a 
href="#Xtapaswi2016movieqa">Tapaswi et al.</a>&#x00A0;<a 
href="#Xtapaswi2016movieqa">2016</a>), whereas a
plethora of work in marketing on motion pictures (<a 
href="#Xeliashberg2006motion">Eliashberg, Elberse and Leenders</a>&#x00A0;<a 
href="#Xeliashberg2006motion">2006</a>) appear
more result-oriented. By combining the best of both disciplines, a deeper understanding could be
achieved about which aspects of movies drive which aspects of consumer response, as well market
outcomes.
<!--l. 1212--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">5.3   </span> <a 
 id="x1-210005.3"></a>Fashion</h4>
<!--l. 1214--><p class="noindent" >Studies about fashion style and related recommendation systems abound in both disciplines.
However, the two bodies of literature adopt almost exclusively different methodologies &#8212; there
appear to be more machine learning and deep learning in fashion studies in computer vision (<a 
href="#Xchao2009framework">Chao
et al.</a>&#x00A0;<a 
href="#Xchao2009framework">2009</a>,&#x00A0;<a 
href="#Xvittayakorn2015runway">Vittayakorn et al.</a>&#x00A0;<a 
href="#Xvittayakorn2015runway">2015</a>), and more time-series analysis and discrete choice
models in fashion studies in marketing (<a 
href="#Xyoganarasimhan2017identifying">Yoganarasimhan</a>&#x00A0;<a 
href="#Xyoganarasimhan2017identifying">2017</a>,&#x00A0;<a 
href="#Xdesign_shi_2018">Shi, Lee, Singh and
Srinivasan</a>&#x00A0;<a 
href="#Xdesign_shi_2018">2018</a>). We think that computer vision methods could be combined with time-series
analysis for more holistic analyses of fashion phenomena. Additionally, similar fashion
recommendation systems have been proposed separately by both marketing scholars
(<a 
href="#Xlu2016video">Lu et al.</a>&#x00A0;<a 
href="#Xlu2016video">2016</a>) and vision scientists (<a 
href="#Xchao2009framework">Chao et al.</a>&#x00A0;<a 
href="#Xchao2009framework">2009</a>,&#x00A0;<a 
href="#Xmcauley2015image">McAuley et al.</a>&#x00A0;<a 
href="#Xmcauley2015image">2015</a>). We
believe that future work could benefit from more exploration from the source of the
fashion industry &#8212; designers. For instance, generative models that inherit the idea of
perfectly balancing style and substance from the body of literature on artistic style
transfer (<a 
href="#Xgatys2015neural">Gatys et al.</a>&#x00A0;<a 
href="#Xgatys2015neural">2015</a>) reviewed in Section&#x00A0;<a 
href="#x1-160003.6.2">3.6.2<!--tex4ht:ref: subsubsec-style-transfer --></a> could be applied to the realms of
fashion design, logo design, architecture design, interior design, etc. Furthermore, when
combined with market outcome data collected from end consumers, such automatic
design generation systems could be promising and useful for marketing practitioners and
consumers alike. <a 
href="#Xtoubia3">Tkachenko et al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xtoubia3">2018</a></span>) appears to be one notable step towards this
direction.
                                                                                         
                                                                                         
<!--l. 1237--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">5.4   </span> <a 
 id="x1-220005.4"></a>Faces, emotions, postures, and more</h4>
<!--l. 1239--><p class="noindent" >Face, emotion, posture detection studies abound in computer vision, whereas studies revolving
these topics in marketing have traditionally been restricted to small sample size, controlled
laboratory experiments using college students or AMT workers as subjects. Due to
the almost mutually exclusive research objectives &#8212; detection accuracy for computer
vision studies and market outcome for marketing studies &#8212; there exists few overlapping
work despite being well explored in both disciplines. We propose a more end-to-end
automatic system starting from large-scale unstructured data to the ultimate measures of
market outcomes that integrate both streams of literature, with studies in computer
vision supporting the front end and those in marketing providing intermediate pipelines
and connections to the ultimate market outcome variables for research questions such
as:
      <ul class="itemize1">
      <li class="itemize">&#8220;which aspects of reality TV shows invoke the greatest consumer engagement?&#8221;
      </li>
      <li class="itemize">&#8220;what kinds of election campaign video clips gather the greatest support from the
      audience?&#8221;
      </li>
      <li class="itemize">&#8220;how to automatically generate the most persuasive business pitch that gets the largest
      amount of investment given the product idea?&#8221;
      </li>
      <li class="itemize">&#8220;could we automatically generate enticing advertisements that lead to the greatest
      purchase intention given a particular consumer profile?&#8221;</li></ul>
<!--l. 1265--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">6.   </span> <a 
 id="x1-230006"></a>Other Potential Future Work</h3>
                                                                                         
                                                                                         <!--l. 1269--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.1   </span> <a 
 id="x1-240006.1"></a>Language and Vision</h4>
<!--l. 1271--><p class="noindent" >There is a growing body of research at the intersection of natural language processing and
computer vision. Some representative work include <a 
href="#Xfrome2013devise">Frome, Corrado, Shlens, Bengio, Dean, Mikolov
et&#x00A0;al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xfrome2013devise">2013</a></span>) and <a 
href="#Xkarpathy2015deep">Karpathy and Fei-Fei</a>&#x00A0;(<span class="bib-year"><a 
href="#Xkarpathy2015deep">2015</a></span>) that learn deep visual-semantic embedding
models for tasks such as generating textual descriptions of images and constituents
therein. We believe such multimodal learning frameworks could be especially valuable to
marketing research due to the fact that visual marketing and content marketing go
hand in hand. In previous research, the effects of certain linguistic subtleties and visual
stimuli have always been separately studies, we believe that an integrated framework
borrowing from the emerging body of research on language and vision could pay great
dividends.
<!--l. 1284--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.2   </span> <a 
 id="x1-250006.2"></a>Fine-grained Classification</h4>
<!--l. 1286--><p class="noindent" >The world is long-tailed. What this means for computer vision and visual recognition is two-fold:
(1) the number of categories to consider in applications can be very large, and (2) the number of
training examples for most categories can be very small. Such challenges are exactly what
<span 
class="cmti-12">fine-grained classification </span>problems pose and there appears to be an ever growing body of literature
tackling fine-grained classification problems of various aspects, whether it be classifying
birds (<a 
href="#Xberg2014birdsnap">Berg, Liu, Woo&#x00A0;Lee, Alexander, Jacobs and Belhumeur</a>&#x00A0;<a 
href="#Xberg2014birdsnap">2014</a>,&#x00A0;<a 
href="#Xvan2017devil">Van&#x00A0;Horn and
Perona</a>&#x00A0;<a 
href="#Xvan2017devil">2017</a>), named entities (<a 
href="#Xfleischman2002fine">Fleischman and Hovy</a>&#x00A0;<a 
href="#Xfleischman2002fine">2002</a>), cars (<a 
href="#Xyang2015large">Yang, Luo, Change&#x00A0;Loy and
Tang</a>&#x00A0;<a 
href="#Xyang2015large">2015</a>,&#x00A0;<a 
href="#Xgebru2017fine">Gebru, Krause, Wang, Chen, Deng and Fei-Fei</a>&#x00A0;<a 
href="#Xgebru2017fine">2017</a>), aircrafts (<a 
href="#Xmaji2013fine">Maji, Rahtu,
Kannala, Blaschko and Vedaldi</a>&#x00A0;<a 
href="#Xmaji2013fine">2013</a>), cooking activities (<a 
href="#Xrohrbach2012database">Rohrbach, Amin, Andriluka and
Schiele</a>&#x00A0;<a 
href="#Xrohrbach2012database">2012</a>), dog breeds (<a 
href="#Xliu2012dog">Liu, Kanazawa, Jacobs and Belhumeur</a>&#x00A0;<a 
href="#Xliu2012dog">2012a</a>), and so forth.
We believe that such classification frameworks and methods are suitable for marketing
applications, especially with respect to the microfoundations of consumer preferences. <a 
href="#Xgebru2017fine">Gebru et
al.</a>&#x00A0;(<span class="bib-year"><a 
href="#Xgebru2017fine">2017</a></span>) showcase how fine-grained classification could be especially cost-effective
and time-saving for large-scale socio-economic demographic estimation and therefore
facilitate public economic policy implementations. For instance, fine-grained classification
methods could be used to pick up subtle differences in shopping environments, brand
images, product designs, logo designs, etc., automatically at a large scale, which enables
                                                                                         
                                                                                         
marketers to segment consumers into even finer categories and devise better targeting
strategies.
<!--l. 1308--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.3   </span> <a 
 id="x1-260006.3"></a>3D Reconstruction and Virtual Reality</h4>
<!--l. 1310--><p class="noindent" >3D reconstruction has been a traditional topic in computer vision. With the rise of virtual reality
and augmented reality applications, it appears to have witnessed regained popularity
over time. Some more recent interesting applications include reconstructing landmarks
(<a 
href="#Xagarwal2009building">Agarwal, Snavely, Simon, Seitz and Szeliski</a>&#x00A0;<a 
href="#Xagarwal2009building">2009</a>,&#x00A0;<a 
href="#Xfrahm2010building">Frahm, Fite-Georgel, Gallup, Johnson,
Raguram, Wu, Jen, Dunn, Clipp, Lazebnik et&#x00A0;al.</a>&#x00A0;<a 
href="#Xfrahm2010building">2010</a>,&#x00A0;<a 
href="#Xsnavely2006photo">Snavely, Seitz and Szeliski</a>&#x00A0;<a 
href="#Xsnavely2006photo">2006</a>),
social scenes (<a 
href="#Xsnavely2008modeling">Snavely, Seitz and Szeliski</a>&#x00A0;<a 
href="#Xsnavely2008modeling">2008</a>), buildings such as museums and galleries
(<a 
href="#Xxiao2014reconstructing">Xiao and Furukawa</a>&#x00A0;<a 
href="#Xxiao2014reconstructing">2014</a>), apartment layouts (<a 
href="#Xzou2018layoutnet">Zou, Colburn, Shan and Hoiem</a>&#x00A0;<a 
href="#Xzou2018layoutnet">2018</a>),
and so forth. We believe, with the virtual reality and augmented reality penetrating
every domain of economics and business, that integrating 3D reconstruction methods
into marketing research and applications could shed light on what had long eluded
marketing scholars and practitioners. For instance, which aspects of virtual reality could
lead to greater consumer engagement, result in customer satisfaction, and/or are more
effective in persuading consumers into purchasing? Which dimensions of augmented
reality applications might help or defeat which marketing objectives and/or market
outcomes?
<!--l. 1326--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.4   </span> <a 
 id="x1-270006.4"></a>Multi-task Learning, Multimodal Learning, and Multimedia</h4>
<!--l. 1328--><p class="noindent" >With the rise of ubiquity of unstructured large-scale datasets, as well as unprecedented
advancement in computing power, multimodal and multi-task learning have gained
popularity in the machine/deep learning over the past few decades. Given the fact that
sound/music, visuals, and content go hand in hand in the marketing portfolios of any
brands or companies, it is only natural that such methods be applied to marketing
applications. For example, how could we jointly predict sales, consumer engagement, and
consumer willingness to buy given numerical, textual, auditory, pictorial, and temporal
data?
                                                                                         
                                                                                         
<!--l. 1338--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">7.   </span> <a 
 id="x1-280007"></a>Conclusion and Disclaimer</h3>
<!--l. 1340--><p class="noindent" >In summary, we review studies in marketing using large-scale computer vision and image
processing techniques, as well as papers in computer vision that address marketing and business
problems. We outline the overlap and the gap in-between the separate streams of literature and
identify opportunities for future work. We enumerate recently collected, large-scale, unstructured,
and annotated datasets that have appeared in these studies, followed by our two cents on potential
opportunities given the current state of research streams in both disciplines (marketing and
computer vision). All opinions are our own. And the list of studies reviewed is by no means
exhaustive. Apologies if yours is/are not included and please let me know so that we could revise
promptly.
<!--l. 4--><p class="noindent" >
   <h3 class="likesectionHead"><a 
 id="x1-290007"></a>References</h3>
<!--l. 4--><p class="noindent" >
     <div class="thebibliography">
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xagarwal2009building"></a><span 
class="cmbx-12">Agarwal, Sameer, Noah Snavely, Ian Simon, Steven</span><span 
class="cmbx-12">&#x00A0;M Seitz, and Richard</span>
     <span 
class="cmbx-12">Szeliski</span>, &#8220;Building rome in a day,&#8221; in &#8220;Computer Vision, 2009 IEEE 12th International
     Conference on&#8221; IEEE 2009, pp.&#x00A0;72&#8211;79.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xazimi2012visual"></a><span 
class="cmbx-12">Azimi, Javad, Ruofei Zhang, Yang Zhou, Vidhya Navalpakkam, Jianchang</span>
     <span 
class="cmbx-12">Mao, and Xiaoli Fern</span>, &#8220;Visual appearance of display ads and its effect on click
     through rate,&#8221; in &#8220;Proceedings of the 21st ACM international conference on Information
     and knowledge management&#8221; ACM 2012, pp.&#x00A0;495&#8211;504.
     </p>
                                                                                         
                                                                                         <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xberg2014birdsnap"></a><span 
class="cmbx-12">Berg,  Thomas,  Jiongxin  Liu,  Seung</span><span 
class="cmbx-12">&#x00A0;Woo  Lee,  Michelle</span><span 
class="cmbx-12">&#x00A0;L  Alexander,</span>
     <span 
class="cmbx-12">David</span><span 
class="cmbx-12">&#x00A0;W Jacobs, and Peter</span><span 
class="cmbx-12">&#x00A0;N Belhumeur</span>, &#8220;Birdsnap: Large-scale fine-grained
     visual categorization of birds,&#8221; in &#8220;Proceedings of the IEEE Conference on Computer
     Vision and Pattern Recognition&#8221; 2014, pp.&#x00A0;2011&#8211;2018.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xbossard2012apparel"></a><span 
class="cmbx-12">Bossard, Lukas, Matthias Dantone, Christian Leistner, Christian Wengert,</span>
     <span 
class="cmbx-12">Till  Quack,  and  Luc</span><span 
class="cmbx-12">&#x00A0;Van  Gool</span>,  &#8220;Apparel  classification  with  style,&#8221;  in  &#8220;Asian
     conference on computer vision&#8221; Springer 2012, pp.&#x00A0;321&#8211;335.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xbylinskii2017learning"></a><span 
class="cmbx-12">Bylinskii,  Zoya,  Nam</span><span 
class="cmbx-12">&#x00A0;Wook  Kim,  Peter</span><span 
class="cmbx-12">&#x00A0;O  Donovan,  Sami  Alsheikh,</span>
     <span 
class="cmbx-12">Spandan  Madan,  Hanspeter  Pfister,  Fredo  Durand,  Bryan  Russell,  and</span>
     <span 
class="cmbx-12">Aaron  Hertzmann</span>,  &#8220;Learning  Visual  Importance  for  Graphic  Designs  and  Data
     Visualizations,&#8221; in &#8220;Proceedings of the 30th Annual ACM Symposium on User Interface
     Software &amp; Technology&#8221; 2017.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xbylinskii2017understanding"></a>___ <span 
class="cmbx-12">, Sami Alsheikh, Spandan Madan, Adria Recasens, Kimberli Zhong,</span>
     <span 
class="cmbx-12">Hanspeter Pfister, Fredo Durand, and Aude Oliva</span>, &#8220;Understanding infographics
     through textual and visual tag prediction,&#8221; <span 
class="cmti-12">arXiv preprint arXiv:1709.09215</span>, 2017.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xchan2017styles"></a><span 
class="cmbx-12">Chan,  Tian</span><span 
class="cmbx-12">&#x00A0;Heong,  J</span><span 
class="cmbx-12">ürgen  Mihm,  and  Manuel</span><span 
class="cmbx-12">&#x00A0;E  Sosa</span>,  &#8220;On  Styles  in
     Product Design: An Analysis of US Design Patents,&#8221; <span 
class="cmti-12">Management Science</span>, 2017, <span 
class="cmti-12">64</span>
     (3), 1230&#8211;1249.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xchandrasekaran2016we"></a><span 
class="cmbx-12">Chandrasekaran, Arjun, Ashwin</span><span 
class="cmbx-12">&#x00A0;K Vijayakumar, Stanislaw Antol, Mohit</span>
                                                                                         
                                                                                         
     <span 
class="cmbx-12">Bansal, Dhruv Batra, C</span><span 
class="cmbx-12">&#x00A0;Lawrence Zitnick, and Devi Parikh</span>, &#8220;We are humor
     beings:  Understanding  and  predicting  visual  humor,&#8221;  in  &#8220;Proceedings  of  the  IEEE
     Conference on Computer Vision and Pattern Recognition&#8221; 2016, pp.&#x00A0;4603&#8211;4612.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xchao2009framework"></a><span 
class="cmbx-12">Chao, Xiaofei, Mark</span><span 
class="cmbx-12">&#x00A0;J Huiskes, Tommaso Gritti, and Calina Ciuhu</span>, &#8220;A
     framework for robust feature selection for real-time fashion style recommendation,&#8221; in
     &#8220;Proceedings of the 1st international workshop on Interactive multimedia for consumer
     electronics&#8221; ACM 2009, pp.&#x00A0;35&#8211;42.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xchen2006composite"></a><span 
class="cmbx-12">Chen, Hong, Zi</span><span 
class="cmbx-12">&#x00A0;Jian Xu, Zi</span><span 
class="cmbx-12">&#x00A0;Qiang Liu, and Song</span><span 
class="cmbx-12">&#x00A0;Chun Zhu</span>, &#8220;Composite
     Templates  for  Cloth  Modeling  and  Sketching,&#8221;  in  &#8220;Computer  Vision  and  Pattern
     Recognition (CVPR), 2006 IEEE Conference on&#8221; IEEE 2006.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xcheng2012multimedia"></a><span 
class="cmbx-12">Cheng, Haibin, Roelof van Zwol, Javad Azimi, Eren Manavoglu, Ruofei</span>
     <span 
class="cmbx-12">Zhang, Yang Zhou, and Vidhya Navalpakkam</span>, &#8220;Multimedia features for click
     prediction  of  new  ads  in  display  advertising,&#8221;  in  &#8220;Proceedings  of  the  18th  ACM
     SIGKDD international conference on Knowledge discovery and data mining&#8221; ACM 2012,
     pp.&#x00A0;777&#8211;785.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xchilton2018"></a><span 
class="cmbx-12">Chilton, Lydia</span><span 
class="cmbx-12">&#x00A0;B</span>, &#8220;Constructing Visual Metaphors for Creative Ads,&#8221; 2018.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xchiltonhumortools"></a>____ <span 
class="cmbx-12">,  James</span><span 
class="cmbx-12">&#x00A0;A  Landay,  and  Daniel</span><span 
class="cmbx-12">&#x00A0;S  Weld</span>,  &#8220;HumorTools:  A  Microtask
     Workflow for Writing News Satire,&#8221; 2018.
                                                                                         
                                                                                         </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xlogodew2018"></a><span 
class="cmbx-12">Dew, Ryan, Asim Ansari, and Olivier Toubia</span>, &#8220;Letting Logos Speak: A Machine
     Learning Approach for Data-Driven Logo Design,&#8221; 2018.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xdoersch2012what"></a><span 
class="cmbx-12">Doersch, Carl, Saurabh Singh, Abhinav Gupta, Josef Sivic, and Alexei</span><span 
class="cmbx-12">&#x00A0;A.</span>
     <span 
class="cmbx-12">Efros</span>,  &#8220;What  Makes  Paris  Look  like  Paris?,&#8221;  <span 
class="cmti-12">ACM  Transactions  on  Graphics</span>
     <span 
class="cmti-12">(SIGGRAPH)</span>, 2012, <span 
class="cmti-12">31 </span>(4), 101:1&#8211;101:9.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xdaria2018return"></a><span 
class="cmbx-12">Dzyabura, Daria, Marat Ibragimov, and Siham</span><span 
class="cmbx-12">&#x00A0;El Kihal</span>, 2018.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xeliashberg2006motion"></a><span 
class="cmbx-12">Eliashberg,  Jehoshua,  Anita  Elberse,  and  Mark</span><span 
class="cmbx-12">&#x00A0;AAM  Leenders</span>,  &#8220;The
     motion picture industry: Critical issues in practice, current research, and new research
     directions,&#8221; <span 
class="cmti-12">Marketing science</span>, 2006, <span 
class="cmti-12">25 </span>(6), 638&#8211;661.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xfleischman2002fine"></a><span 
class="cmbx-12">Fleischman, Michael and Eduard Hovy</span>, &#8220;Fine grained classification of named
     entities,&#8221;  in  &#8220;Proceedings  of  the  19th  international  conference  on  Computational
     linguistics-Volume 1&#8221; Association for Computational Linguistics 2002, pp.&#x00A0;1&#8211;7.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xfrahm2010building"></a><span 
class="cmbx-12">Frahm,  Jan-Michael,  Pierre  Fite-Georgel,  David  Gallup,  Tim  Johnson,</span>
     <span 
class="cmbx-12">Rahul  Raguram,  Changchang  Wu,  Yi-Hung  Jen,  Enrique  Dunn,  Brian</span>
     <span 
class="cmbx-12">Clipp, Svetlana Lazebnik et</span><span 
class="cmbx-12">&#x00A0;al.</span>, &#8220;Building rome on a cloudless day,&#8221; in &#8220;European
     Conference on Computer Vision&#8221; Springer 2010, pp.&#x00A0;368&#8211;381.
                                                                                         
                                                                                         </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xfrome2013devise"></a><span 
class="cmbx-12">Frome, Andrea, Greg</span><span 
class="cmbx-12">&#x00A0;S Corrado, Jon Shlens, Samy Bengio, Jeff Dean,</span>
     <span 
class="cmbx-12">Tomas  Mikolov  et</span><span 
class="cmbx-12">&#x00A0;al.</span>,  &#8220;Devise:  A  deep  visual-semantic  embedding  model,&#8221;  in
     &#8220;Advances in neural information processing systems&#8221; 2013, pp.&#x00A0;2121&#8211;2129.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XGarces2014ASM"></a><span 
class="cmbx-12">Garces, Elena, Aseem Agarwala, Diego Gutierrez, and Aaron Hertzmann</span>,
     &#8220;A similarity measure for illustration style,&#8221; <span 
class="cmti-12">ACM Trans. Graph.</span>, 2014, <span 
class="cmti-12">33</span>, 93:1&#8211;93:9.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xgatys2015neural"></a><span 
class="cmbx-12">Gatys,  Leon</span><span 
class="cmbx-12">&#x00A0;A,  Alexander</span><span 
class="cmbx-12">&#x00A0;S  Ecker,  and  Matthias  Bethge</span>,  &#8220;A  neural
     algorithm of artistic style,&#8221; <span 
class="cmti-12">arXiv preprint arXiv:1508.06576</span>, 2015.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xgatys2016image"></a>__ <span 
class="cmbx-12">,</span> _____ <span 
class="cmbx-12">, and</span> _____ , &#8220;Image style transfer using convolutional neural networks,&#8221; in
     &#8220;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition&#8221;
     2016, pp.&#x00A0;2414&#8211;2423.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xgebru2017fine"></a><span 
class="cmbx-12">Gebru, Timnit, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, and</span>
     <span 
class="cmbx-12">Li</span><span 
class="cmbx-12">&#x00A0;Fei-Fei</span>, &#8220;Fine-Grained Car Detection for Visual Census Estimation.,&#8221; in &#8220;AAAI,&#8221;
     Vol.&#x00A0;2 2017, p.&#x00A0;6.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xvan2017devil"></a><span 
class="cmbx-12">Horn, Grant</span><span 
class="cmbx-12">&#x00A0;Van and Pietro Perona</span>, &#8220;The Devil is in the Tails: Fine-grained
     Classification in the Wild,&#8221; <span 
class="cmti-12">arXiv preprint arXiv:1709.01450</span>, 2017.
     </p>
                                                                                         
                                                                                         <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xhuang2016inferring"></a><span 
class="cmbx-12">Huang, Xinyue and Adriana Kovashka</span>, &#8220;Inferring visual persuasion via body
     language,  setting,  and  deep  features,&#8221;  in  &#8220;Proceedings  of  the  IEEE  Conference  on
     Computer Vision and Pattern Recognition Workshops&#8221; 2016, pp.&#x00A0;73&#8211;79.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xhussain2017automatic"></a><span 
class="cmbx-12">Hussain, Zaeem, Mingda Zhang, Xiaozhong Zhang, Keren Ye, Christopher</span>
     <span 
class="cmbx-12">Thomas,  Zuha  Agha,  Nathan  Ong,  and  Adriana  Kovashka</span>,  &#8220;Automatic
     understanding  of  image  and  video  advertisements,&#8221;  in  &#8220;2017  IEEE  Conference  on
     Computer Vision and Pattern Recognition (CVPR)&#8221; IEEE 2017, pp.&#x00A0;1100&#8211;1110.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xjohnson2016perceptual"></a><span 
class="cmbx-12">Johnson,  Justin,  Alexandre  Alahi,  and  Li</span><span 
class="cmbx-12">&#x00A0;Fei-Fei</span>,  &#8220;Perceptual  losses  for
     real-time style transfer and super-resolution,&#8221; in &#8220;European Conference on Computer
     Vision&#8221; Springer 2016, pp.&#x00A0;694&#8211;711.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xjoo2015automated"></a><span 
class="cmbx-12">Joo, Jungseock, Francis</span><span 
class="cmbx-12">&#x00A0;F Steen, and Song-Chun Zhu</span>, &#8220;Automated facial trait
     judgment and election outcome prediction: Social dimensions of face,&#8221; in &#8220;Proceedings
     of the IEEE international conference on computer vision&#8221; 2015, pp.&#x00A0;3712&#8211;3720.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xjoo2014visual"></a>___ <span 
class="cmbx-12">, Weixin Li, Francis</span><span 
class="cmbx-12">&#x00A0;F Steen, and Song-Chun Zhu</span>, &#8220;Visual persuasion:
     Inferring communicative intents of images,&#8221; in &#8220;Proceedings of the IEEE conference on
     computer vision and pattern recognition&#8221; 2014, pp.&#x00A0;216&#8211;223.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xkarayev2013recognizing"></a><span 
class="cmbx-12">Karayev,  Sergey,  Matthew  Trentacoste,  Helen  Han,  Aseem  Agarwala,</span>
     <span 
class="cmbx-12">Trevor Darrell, Aaron Hertzmann, and Holger Winnemoeller</span>, &#8220;Recognizing
     image style,&#8221; <span 
class="cmti-12">arXiv preprint arXiv:1311.3715</span>, 2013.
                                                                                         
                                                                                         </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xkarpathy2015deep"></a><span 
class="cmbx-12">Karpathy, Andrej and Li</span><span 
class="cmbx-12">&#x00A0;Fei-Fei</span>, &#8220;Deep visual-semantic alignments for generating
     image descriptions,&#8221; in &#8220;Proceedings of the IEEE conference on computer vision and
     pattern recognition&#8221; 2015, pp.&#x00A0;3128&#8211;3137.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xkiapour2014hipster"></a><span 
class="cmbx-12">Kiapour, M</span><span 
class="cmbx-12">&#x00A0;Hadi, Kota Yamaguchi, Alexander</span><span 
class="cmbx-12">&#x00A0;C Berg, and Tamara</span><span 
class="cmbx-12">&#x00A0;L</span>
     <span 
class="cmbx-12">Berg</span>, &#8220;Hipster wars: Discovering elements of fashion styles,&#8221; in &#8220;European conference
     on computer vision&#8221; Springer 2014, pp.&#x00A0;472&#8211;488.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XNIPS2012_4824"></a><span 
class="cmbx-12">Krizhevsky,  Alex,  Ilya  Sutskever,  and  Geoffrey</span><span 
class="cmbx-12">&#x00A0;E  Hinton</span>,  &#8220;ImageNet
     Classification  with  Deep  Convolutional  Neural  Networks,&#8221;  in  F.&#x00A0;Pereira,  C.&#x00A0;J.&#x00A0;C.
     Burges,  L.&#x00A0;Bottou,  and  K.&#x00A0;Q.  Weinberger,  eds.,  <span 
class="cmti-12">Advances  in  Neural  Information</span>
     <span 
class="cmti-12">Processing Systems 25</span>, Curran Associates, Inc., 2012, pp.&#x00A0;1097&#8211;1105.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XLee_2013_ICCV"></a><span 
class="cmbx-12">Lee, Yong</span><span 
class="cmbx-12">&#x00A0;Jae, Alexei</span><span 
class="cmbx-12">&#x00A0;A. Efros, and Martial Hebert</span>, &#8220;Style-Aware Mid-level
     Representation for Discovering Visual Connections in Space and Time,&#8221; in &#8220;The IEEE
     International Conference on Computer Vision (ICCV)&#8221; December 2013.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xli2016combining"></a><span 
class="cmbx-12">Li, Chuan and Michael Wand</span>, &#8220;Combining markov random fields and convolutional
     neural  networks  for  image  synthesis,&#8221;  in  &#8220;Proceedings  of  the  IEEE  Conference  on
     Computer Vision and Pattern Recognition&#8221; 2016, pp.&#x00A0;2479&#8211;2486.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xliu2015inferring"></a><span 
class="cmbx-12">Liu, Gaowen, Yan Yan, Elisa Ricci, Yi</span><span 
class="cmbx-12">&#x00A0;Yang, Yahong Han, Stefan Winkler,</span>
                                                                                         
                                                                                         
     <span 
class="cmbx-12">and Nicu Sebe</span>, &#8220;Inferring Painting Style with Multi-Task Dictionary Learning.,&#8221; in
     &#8220;IJCAI&#8221; 2015, pp.&#x00A0;2162&#8211;2168.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xliu2012dog"></a><span 
class="cmbx-12">Liu, Jiongxin, Angjoo Kanazawa, David Jacobs, and Peter Belhumeur</span>, &#8220;Dog
     breed classification using part localization,&#8221; in &#8220;European Conference on Computer
     Vision&#8221; Springer 2012, pp.&#x00A0;172&#8211;185.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xliuliu2018"></a><span 
class="cmbx-12">Liu, Liu and Dina Mayzlin</span>, &#8220;Visual Listening in: Extracting Brand Image Portrayed
     on Social Media,&#8221; 2018.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xliu2012street"></a><span 
class="cmbx-12">Liu, Si, Zheng Song, Guangcan Liu, Changsheng Xu, Hanqing Lu, and</span>
     <span 
class="cmbx-12">Shuicheng Yan</span>, &#8220;Street-to-shop: Cross-scenario clothing retrieval via parts alignment
     and auxiliary set,&#8221; in &#8220;Computer Vision and Pattern Recognition (CVPR), 2012 IEEE
     Conference on&#8221; IEEE 2012, pp.&#x00A0;3330&#8211;3337.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xlu2016video"></a><span 
class="cmbx-12">Lu, Shasha, Li</span><span 
class="cmbx-12">&#x00A0;Xiao, and Min Ding</span>, &#8220;A video-based automated recommender
     (VAR) system for garments,&#8221; <span 
class="cmti-12">Marketing Science</span>, 2016, <span 
class="cmti-12">35 </span>(3), 484&#8211;510.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xluan2017deep"></a><span 
class="cmbx-12">Luan, Fujun, Sylvain Paris, Eli Shechtman, and Kavita Bala</span>, &#8220;Deep Photo
     Style Transfer,&#8221; <span 
class="cmti-12">arXiv preprint arXiv:1703.07511</span>, 2017.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xmaji2013fine"></a><span 
class="cmbx-12">Maji,  Subhransu,  Esa  Rahtu,  Juho  Kannala,  Matthew  Blaschko,  and</span>
                                                                                         
                                                                                         
     <span 
class="cmbx-12">Andrea  Vedaldi</span>,  &#8220;Fine-grained  visual  classification  of  aircraft,&#8221;  <span 
class="cmti-12">arXiv  preprint</span>
     <span 
class="cmti-12">arXiv:1306.5151</span>, 2013.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xbeautylee2018"></a><span 
class="cmbx-12">Malik, Nikhil, Param</span><span 
class="cmbx-12">&#x00A0;Vir Singh, Dokyun Lee, and Kannan Srinivasan</span>,
     &#8220;When Does Beauty Pay. A Large Scale Image Based Appearance Analysis on Career
     Transitions,&#8221; 2018.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xmcauley2015image"></a><span 
class="cmbx-12">McAuley, Julian, Christopher Targett, Qinfeng Shi, and Anton Van</span><span 
class="cmbx-12">&#x00A0;Den</span>
     <span 
class="cmbx-12">Hengel</span>, &#8220;Image-based recommendations on styles and substitutes,&#8221; in &#8220;Proceedings
     of the 38th International ACM SIGIR Conference on Research and Development in
     Information Retrieval&#8221; ACM 2015, pp.&#x00A0;43&#8211;52.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xmcduff2015predicting"></a><span 
class="cmbx-12">McDuff,  Daniel,  Rana</span><span 
class="cmbx-12">&#x00A0;El  Kaliouby,  Jeffrey</span><span 
class="cmbx-12">&#x00A0;F  Cohn,  and  Rosalind</span><span 
class="cmbx-12">&#x00A0;W</span>
     <span 
class="cmbx-12">Picard</span>,  &#8220;Predicting  ad  liking  and  purchase  intent:  Large-scale  analysis  of  facial
     responses to ads,&#8221; <span 
class="cmti-12">IEEE Transactions on Affective Computing</span>, 2015, <span 
class="cmti-12">6 </span>(3), 223&#8211;235.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xmei2012imagesense"></a><span 
class="cmbx-12">Mei,  Tao,  Lusong  Li,  Xian-Sheng  Hua,  and  Shipeng  Li</span>,  &#8220;ImageSense:
     Towards contextual image advertising,&#8221; <span 
class="cmti-12">ACM Transactions on Multimedia Computing,</span>
     <span 
class="cmti-12">Communications, and Applications (TOMM)</span>, 2012, <span 
class="cmti-12">8 </span>(1), 6.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xmurillo2012urban"></a><span 
class="cmbx-12">Murillo, Ana</span><span 
class="cmbx-12">&#x00A0;C, Iljung</span><span 
class="cmbx-12">&#x00A0;S Kwak, Lubomir Bourdev, David Kriegman, and</span>
     <span 
class="cmbx-12">Serge Belongie</span>, &#8220;Urban tribes: Analyzing group photos from a social perspective,&#8221;
     in  &#8220;Computer  Vision  and  Pattern  Recognition  Workshops  (CVPRW),  2012  IEEE
     Computer Society Conference on&#8221; IEEE 2012, pp.&#x00A0;28&#8211;35.
                                                                                         
                                                                                         </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xpapatla2018"></a><span 
class="cmbx-12">Papatla, Purushottam</span>, &#8220;Face, Body or Both? Effects of Partial and Full Visibility
     of People in VUGC on Consumer Response,&#8221; 2018.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XQuercia2014AestheticCW"></a><span 
class="cmbx-12">Quercia, Daniele, Neil</span><span 
class="cmbx-12">&#x00A0;Keith O&#8217;Hare, and Henriette Cramer</span>, &#8220;Aesthetic
     capital: what makes london look beautiful, quiet, and happy?,&#8221; in &#8220;CSCW&#8221; 2014.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xrohrbach2015dataset"></a><span 
class="cmbx-12">Rohrbach, Anna, Marcus Rohrbach, Niket Tandon, and Bernt Schiele</span>, &#8220;A
     dataset for movie description,&#8221; in &#8220;Proceedings of the IEEE conference on computer
     vision and pattern recognition&#8221; 2015, pp.&#x00A0;3202&#8211;3212.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xrohrbach2012database"></a><span 
class="cmbx-12">Rohrbach, Marcus, Sikandar Amin, Mykhaylo Andriluka, and Bernt Schiele</span>,
     &#8220;A database for fine grained activity detection of cooking activities,&#8221; in &#8220;Computer
     Vision  and  Pattern  Recognition  (CVPR),  2012  IEEE  Conference  on&#8221;  IEEE  2012,
     pp.&#x00A0;1194&#8211;1201.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xsanchez2002shot"></a><span 
class="cmbx-12">S</span><span 
class="cmbx-12">ánchez, Juan</span><span 
class="cmbx-12">&#x00A0;M, Xavier Binefa, and Jordi Vitri</span><span 
class="cmbx-12">à</span>, &#8220;Shot partitioning based
     recognition  of  tv  commercials,&#8221;  <span 
class="cmti-12">Multimedia  Tools  and  Applications</span>,  2002,  <span 
class="cmti-12">18  </span>(3),
     233&#8211;247.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xselim2016painting"></a><span 
class="cmbx-12">Selim, Ahmed, Mohamed Elgharib, and Linda Doyle</span>, &#8220;Painting style transfer
     for head portraits using convolutional neural networks,&#8221; <span 
class="cmti-12">ACM Transactions on Graphics</span>
     <span 
class="cmti-12">(ToG)</span>, 2016, <span 
class="cmti-12">35 </span>(4), 129.
                                                                                         
                                                                                         </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xdesign_shi_2018"></a><span 
class="cmbx-12">Shi, Zijun</span><span 
class="cmbx-12">&#x00A0;(June), Dokyun Lee, Param</span><span 
class="cmbx-12">&#x00A0;Vir Singh, and Kannan Srinivasan</span>,
     &#8220;Design of Fashion: Can Brand Value be Separated from Style Value?,&#8221; 2018.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xsnavely2006photo"></a><span 
class="cmbx-12">Snavely,  Noah,  Steven</span><span 
class="cmbx-12">&#x00A0;M  Seitz,  and  Richard  Szeliski</span>,  &#8220;Photo  tourism:
     exploring photo collections in 3D,&#8221; in &#8220;ACM transactions on graphics (TOG),&#8221; Vol.&#x00A0;25
     ACM 2006, pp.&#x00A0;835&#8211;846.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xsnavely2008modeling"></a>____ <span 
class="cmbx-12">,</span>  _____ <span 
class="cmbx-12">,  and</span>  _____ ,  &#8220;Modeling  the  world  from  internet  photo  collections,&#8221;
     <span 
class="cmti-12">International journal of computer vision</span>, 2008, <span 
class="cmti-12">80 </span>(2), 189&#8211;210.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xtapaswi2016movieqa"></a><span 
class="cmbx-12">Tapaswi, Makarand, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba,</span>
     <span 
class="cmbx-12">Raquel Urtasun, and Sanja Fidler</span>, &#8220;Movieqa: Understanding stories in movies
     through  question-answering,&#8221;  in  &#8220;Proceedings  of  the  IEEE  conference  on  computer
     vision and pattern recognition&#8221; 2016, pp.&#x00A0;4631&#8211;4640.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xthomas2016seeing"></a><span 
class="cmbx-12">Thomas,  Christopher  and  Adriana  Kovashka</span>,  &#8220;Seeing  behind  the  camera:
     Identifying the authorship of a photograph,&#8221; in &#8220;Proceedings of the IEEE Conference
     on Computer Vision and Pattern Recognition&#8221; 2016, pp.&#x00A0;3494&#8211;3502.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xtoubia3"></a><span 
class="cmbx-12">Tkachenko,  Yegor,  Asim  Ansari,  and  Olivier  Toubia</span>,  &#8220;Computer-aided
     Exploration Of Product Designs in High-dimensional Visual Spaces,&#8221; 2018.
                                                                                         
                                                                                         </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xtodorov2018"></a><span 
class="cmbx-12">Todorov, Alexander</span>, &#8220;Modeling Visual Impressions of Faces,&#8221; 2018.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xveit2015learning"></a><span 
class="cmbx-12">Veit, Andreas, Balazs Kovacs, Sean Bell, Julian McAuley, Kavita Bala,</span>
     <span 
class="cmbx-12">and  Serge  Belongie</span>,  &#8220;Learning  visual  clothing  style  with  heterogeneous  dyadic
     co-occurrences,&#8221; in &#8220;Proceedings of the IEEE International Conference on Computer
     Vision&#8221; 2015, pp.&#x00A0;4642&#8211;4650.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xvittayakorn2015runway"></a><span 
class="cmbx-12">Vittayakorn, Sirion, Kota Yamaguchi, Alexander</span><span 
class="cmbx-12">&#x00A0;C Berg, and Tamara</span><span 
class="cmbx-12">&#x00A0;L</span>
     <span 
class="cmbx-12">Berg</span>, &#8220;Runway to realway: Visual analysis of fashion,&#8221; in &#8220;Applications of Computer
     Vision (WACV), 2015 IEEE Winter Conference on&#8221; IEEE 2015, pp.&#x00A0;951&#8211;958.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xwilber2017bam"></a><span 
class="cmbx-12">Wilber,  Michael</span><span 
class="cmbx-12">&#x00A0;J,  Chen  Fang,  Hailin  Jin,  Aaron  Hertzmann,  John</span>
     <span 
class="cmbx-12">Collomosse, and Serge</span><span 
class="cmbx-12">&#x00A0;J Belongie</span>, &#8220;BAM! The Behance Artistic Media Dataset
     for Recognition Beyond Photography.,&#8221; in &#8220;ICCV&#8221; 2017, pp.&#x00A0;1211&#8211;1220.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xxiao2014reconstructing"></a><span 
class="cmbx-12">Xiao, Jianxiong and Yasutaka Furukawa</span>, &#8220;Reconstructing the worldâs museums,&#8221;
     <span 
class="cmti-12">International journal of computer vision</span>, 2014, <span 
class="cmti-12">110 </span>(3), 243&#8211;258.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XXiao2014JustTF"></a><span 
class="cmbx-12">Xiao, Li and Min Ding</span>, &#8220;Just the Faces: Exploring the Effects of Facial Features
     in Print Advertising,&#8221; <span 
class="cmti-12">Marketing Science</span>, 2014, <span 
class="cmti-12">33</span>, 338&#8211;352.
     </p>
                                                                                         
                                                                                         <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xyadati2014cavva"></a><span 
class="cmbx-12">Yadati,  Karthik,  Harish  Katti,  and  Mohan  Kankanhalli</span>,  &#8220;CAVVA:
     Computational affective video-in-video advertising,&#8221; <span 
class="cmti-12">IEEE Transactions on Multimedia</span>,
     2014, <span 
class="cmti-12">16 </span>(1), 15&#8211;23.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xyamaguchi2013paper"></a><span 
class="cmbx-12">Yamaguchi,  Kota,  M</span><span 
class="cmbx-12">&#x00A0;Hadi  Kiapour,  and  Tamara</span><span 
class="cmbx-12">&#x00A0;L  Berg</span>,  &#8220;Paper  doll
     parsing: Retrieving similar styles to parse clothing items,&#8221; in &#8220;Proceedings of the IEEE
     international conference on computer vision&#8221; 2013, pp.&#x00A0;3519&#8211;3526.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xyamaguchi2012parsing"></a>__ <span 
class="cmbx-12">, M</span><span 
class="cmbx-12">&#x00A0;Hadi Kiapour, Luis</span><span 
class="cmbx-12">&#x00A0;E Ortiz, and Tamara</span><span 
class="cmbx-12">&#x00A0;L Berg</span>, &#8220;Parsing clothing
     in fashion photographs,&#8221; in &#8220;Computer Vision and Pattern Recognition (CVPR), 2012
     IEEE Conference on&#8221; IEEE 2012, pp.&#x00A0;3570&#8211;3577.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xyang2015large"></a><span 
class="cmbx-12">Yang, Linjie, Ping Luo, Chen</span><span 
class="cmbx-12">&#x00A0;Change Loy, and Xiaoou Tang</span>, &#8220;A large-scale
     car dataset for fine-grained categorization and verification,&#8221; in &#8220;Proceedings of the IEEE
     Conference on Computer Vision and Pattern Recognition&#8221; 2015, pp.&#x00A0;3973&#8211;3981.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xyang2011articulated"></a><span 
class="cmbx-12">Yang,  Yi  and  Deva  Ramanan</span>,  &#8220;Articulated  pose  estimation  with  flexible
     mixtures-of-parts,&#8221; in &#8220;Computer Vision and Pattern Recognition (CVPR), 2011 IEEE
     Conference on&#8221; IEEE 2011, pp.&#x00A0;1385&#8211;1392.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xyoganarasimhan2017identifying"></a><span 
class="cmbx-12">Yoganarasimhan, Hema</span>, &#8220;Identifying the presence and cause of fashion cycles in
     data,&#8221; <span 
class="cmti-12">Journal of Marketing Research</span>, 2017, <span 
class="cmti-12">54 </span>(1), 5&#8211;26.
                                                                                         
                                                                                         </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xzhanglee2018"></a><span 
class="cmbx-12">Zhang, Shunyuan, Dokyun Lee, Param</span><span 
class="cmbx-12">&#x00A0;Vir Singh, and Kannan Srinivasan</span>,
     &#8220;How Much is an Image Worth? The Impact of Professional versus Amateur Airbnb
     Property Images on Property Demand,&#8221; 2018.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xzhao2011discovering"></a><span 
class="cmbx-12">Zhao, Gangqiang, Junsong Yuan, Jiang Xu, and Ying Wu</span>, &#8220;Discovering the
     thematic object in commercial videos,&#8221; <span 
class="cmti-12">IEEE MultiMedia</span>, 2011, <span 
class="cmti-12">18 </span>(3), 56&#8211;65.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xzhou2016face"></a><span 
class="cmbx-12">Zhou, YingHui, Shasha Lu, and Min Ding</span>, &#8220;A Face Anonymity-Perceptibility
     Paradigm and an Application in the Online Dating Industry,&#8221; 2016.
     </p>
     <p class="bibitem" ><span class="biblabel">
     <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xzou2018layoutnet"></a><span 
class="cmbx-12">Zou,  Chuhang,  Alex  Colburn,  Qi</span><span 
class="cmbx-12">&#x00A0;Shan,  and  Derek  Hoiem</span>,  &#8220;LayoutNet:
     Reconstructing the 3D Room Layout from a Single RGB Image,&#8221; in &#8220;Proceedings of the
     IEEE Conference on Computer Vision and Pattern Recognition&#8221; 2018, pp.&#x00A0;2051&#8211;2059.
</p>
     </div>
    
</body></html>                                                                                  
