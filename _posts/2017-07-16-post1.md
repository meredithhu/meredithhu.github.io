---
layout: post
title: Sanity Checks
date: 2017-07-16
---


1. How is the theoretical framework different from Ely et al? (1) Either following it exactly which I think might not be doable exactly due to theory assumptions (common knowledge for conveying the information; the realization of the signal is truthfully communicated to audience; etc.) (2) identify hypotheses that might be at least partially explained by Ely et al. 

2. How close is the empirical work to the moment by moment studies of ads etc in marketing: (number of genres common in marketing literature: ~20) MTM studies test the effect of MTM X on overall perceived X, where X (numeric) could be humor rating, satisfaction rating, etc.; This study tests the effect of information manifest in scripts on overall evaluation...

3. What is the justification for studying both emotions which include surprise/suspect and info-theoretic measures of surprise/suspect? The former relates to the suspense/surprise manifest in words, the latter in how the story was told throughout...

4. What is the difference between behavioral and cognitive/ emotional info?  (page 12). Visible (volition, kinetic, completed) vs invisible (emotions evoked by words and phrases) Are there better terms for this? Direct vs. indirect; transitive vs. evoked; informative vs. emotional; etc.

5. I don’t understand the discussion of clause transitivity.  Since the ultimate measure is based on SVM for movies with spoilers based on word count, where are clauses being measured? ah I was just following the discussions/interpretations in Boyd-Graber et al. (2013) and Madnani et al. (2010)...

6. How many movies of 1088 have spoiler alerts? How many spoilers per movie - I assume only 1? mhm? I measured info for every script...


7. Are spoilers for surprise or suspense or both? They are for both... They represent the evolution of more important information over time...


8. Other emotions - these measures based on word count only? : uh, recent studies on emotion/psychological language have used both dictionary-based and topic-based approaches (for instance, studies at Penn World Well Being Project by Ungar, Sligman, Schwartz, etc.). back then I did not used topic-based approach because of the working paper of Toubia et al., in which supervised LDA was used to extract personality clusters/topics...

9. On page 20 - 94% of all trajectories fail to reject null of martingale (5% sig). what about 100% of all trajectories? Ah - there were a couple trajectories where the null was rejected... What further analysis should be done here? I also don’t understand why unit of observation is not movie instead of movie-evoked emotion. Because the theory assumed the audience are Bayesian and therefore their beliefs should follow martingales??

10. Section 5.2.2 - not sure I understand the justification for why gripping and other words for surprise/suspense are the best way to identify movies.   Is there some justification for using this?  currently seems ad hoc (even if might be right) Uh, yes this is indeed a problem I think too ad hoc... Otherwise, cluster words with such seed words??

11. I can’t tell whether 53.4% agreement is good or not (page 21): not particularly good I think... but then one approach that might make this look better on paper is to show that the task is intrisically subjective and ambiguous, possibly by asking more mTurkers to rate the level of suspense/surprise given a scene discription...?


12. Page 22 -  can you please explain in a table to me what variables are there in literature that you haven’t looked at-  e.g. screens, awards received by cast in previous movies, etc.

|Variable   |Reference  |
|:----------|:----------|
|price level|Wei (2016) |
|network metrics|Wei (2016)|
|awards|Toubia (2016)|
|#screens,total tv ad spend|Orhun (2016)|
|market size |Wei (2016) |
|production start date|Wei (2016) |


13. Page 26 -  what is the problem being solved and how is it being solved by kernel matching estimator and nearest neighbor?   i.e. why is original analysis not robust enough? Ah I thought it was like selection on observables --- scripts with higher levels of behavioral/cognitive (excuse the terms...) scores are driven by observable differences alreadya and I followed papers that have used both simple logit models and matching estimators to address this.

14. Endogeneity - not just because of decision endogeneity but unobservables (omitted variables) being systematically correlated with observables e.g. advertising levels for movies.  The fixes you have won’t cure this.   usually people use rival movie levels or previous time periods’ movie characteristics. Ah yes thank you for this!! BLP-ish IVs should be used here: 

15. Papers marked in bibliog -  can you get me these in dropbox: <a href="https://www.dropbox.com/sh/54itqbfn4nyk9wa/AABpRKY17JyoCmTPcHUqfMcXa?dl=0">Done</a>.
